{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41539dd",
   "metadata": {},
   "source": [
    "# üéâ Joinnus Events Extraction Pipeline\n",
    "\n",
    "**Complete end-to-end pipeline for extracting comprehensive event data from Joinnus.**\n",
    "\n",
    "## üéØ Pipeline Features\n",
    "\n",
    "This notebook provides a complete workflow:\n",
    "1. **üåê Event URL Extraction** - Extract all event URLs from Joinnus categories with pagination\n",
    "2. **\udcbe Data Storage** - Save extracted URLs to JSON and CSV files in `data/` directory\n",
    "3. **üîç Comprehensive Extraction** - Extract detailed event information from each event page\n",
    "4. **\udcca Results** - Generate detailed JSON and CSV files with all event data\n",
    "\n",
    "## üìã Output Files\n",
    "\n",
    "- `events_TIMESTAMP.csv` - URLs and basic info for all 603+ events\n",
    "- `events_TIMESTAMP.json` - Detailed event data with images, tags, prices\n",
    "- `events_detailed_TIMESTAMP.csv` - Summary view of all extracted data\n",
    "- `events_detailed_TIMESTAMP.json` - Complete detailed data for all events\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "1. Run **Step 1** to extract all event URLs\n",
    "2. Run **Step 2** to extract detailed event information from those URLs\n",
    "3. All results saved to `data/` folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c3f89",
   "metadata": {},
   "source": [
    "## üìö Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for web scraping, data processing, and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1255277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n",
      "üîß Setting up logging configuration...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from html import unescape\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError, ConnectionFailure\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "print(\"üîß Setting up logging configuration...\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cffa6e",
   "metadata": {},
   "source": [
    "## üîß Configuration Setup\n",
    "\n",
    "Configure scraping parameters, directories, and settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c086fe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded\n",
      "üìÇ Data directory: c:\\Scrapping\\joinnus\\notebook\\data\n",
      "üóÑÔ∏è  MongoDB: recommendations-system.events\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "JOINNUS_CONFIG = {\n",
    "    'base_domain': 'https://www.joinnus.com',\n",
    "    'classic_domain': 'https://classic.joinnus.com',\n",
    "    'headers': {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    }\n",
    "}\n",
    "\n",
    "URLS_CONFIG = {\n",
    "    'csv_file': Path('../notebook/urls.csv'),\n",
    "}\n",
    "\n",
    "SCRAPING_CONFIG = {\n",
    "    'timing': {\n",
    "        'page_load_wait': 10,\n",
    "        'element_wait': 5,\n",
    "    },\n",
    "    'pagination': {\n",
    "        'max_pages': 100,\n",
    "    },\n",
    "    'browser': {\n",
    "        'headless': True,\n",
    "        'disable_gpu': True,\n",
    "        'window_size': '1920,1080',\n",
    "        'disable_images': False\n",
    "    },\n",
    "}\n",
    "\n",
    "# MongoDB Configuration\n",
    "MONGODB_CONFIG = {\n",
    "    'uri': 'mongodb+srv://mrclpgg_db_user:K9NMlwFHZpeltCwI@cluster0.qdopesi.mongodb.net/?appName=Cluster0',\n",
    "    'database': 'recommendations-system',\n",
    "    'collection': 'events',\n",
    "}\n",
    "\n",
    "# Data directory in same location as notebook\n",
    "NOTEBOOK_DIR = Path(__file__).parent if '__file__' in dir() else Path.cwd()\n",
    "DATA_DIR = NOTEBOOK_DIR / 'data'\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"üìÇ Data directory: {DATA_DIR}\")\n",
    "print(f\"üóÑÔ∏è  MongoDB: {MONGODB_CONFIG['database']}.{MONGODB_CONFIG['collection']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68647cf",
   "metadata": {},
   "source": [
    "## ‚úÖ Working Pagination Logic\n",
    "\n",
    "This is the **proven pagination logic** that correctly handles numbered page buttons (1, 2, 3, etc.) instead of generic prev/next buttons. It tracks event IDs to detect duplicates and knows when pagination is complete.\n",
    "\n",
    "Key features:\n",
    "- **Finds numbered page buttons** (e.g., button with text \"2\" for page 2)\n",
    "- **Compares event IDs** between pages to detect when we've reached the end\n",
    "- **Uses JavaScript click** for reliable button activation\n",
    "- **8-second wait** for AJAX page load to complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b772a532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pagination function defined - ready to use\n",
      "   Use: events = paginate_category(driver, category_name, extract_func)\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ WORKING PAGINATION LOGIC\n",
    "# This logic correctly handles pagination by:\n",
    "# 1. Looking for numbered page buttons (1, 2, 3, etc.)\n",
    "# 2. Comparing event IDs between pages to detect duplicates\n",
    "# 3. Stopping when no new events are found\n",
    "\n",
    "def paginate_category(driver, category_name, extract_event_urls_func):\n",
    "    \"\"\"\n",
    "    Paginate through all pages of a category and extract event URLs.\n",
    "    \n",
    "    Parameters:\n",
    "    - driver: Selenium WebDriver\n",
    "    - category_name: Name of the category\n",
    "    - extract_event_urls_func: Function to extract events from HTML\n",
    "    \n",
    "    Returns:\n",
    "    - List of all events from all pages\n",
    "    \"\"\"\n",
    "    all_events = []\n",
    "    \n",
    "    try:\n",
    "        # Extract events from first page\n",
    "        events = extract_event_urls_func(driver.page_source, category_name)\n",
    "        print(f\"   Found {len(events)} events on page 1\")\n",
    "        all_events.extend(events)\n",
    "        \n",
    "        # Track previous page event IDs for comparison\n",
    "        previous_ids = set(e['id'] for e in events)\n",
    "        \n",
    "        # Pagination loop - iterate through numbered page buttons\n",
    "        page = 2\n",
    "        while True:\n",
    "            try:\n",
    "                # Find all pagination buttons\n",
    "                pag_buttons = driver.find_elements(By.XPATH, \"//div[contains(@class, 'space-x-2')]//button\")\n",
    "                \n",
    "                # Buttons are: [prev, 1, 2, 3, 4, ..., next]\n",
    "                # We need to find the button with text matching current page number\n",
    "                page_btn = None\n",
    "                for btn in pag_buttons:\n",
    "                    btn_text = btn.text.strip()\n",
    "                    if btn_text == str(page):\n",
    "                        page_btn = btn\n",
    "                        break\n",
    "                \n",
    "                if not page_btn:\n",
    "                    # No button for this page number, we've reached the end\n",
    "                    print(f\"   Page {page}: Button not found - reached end\")\n",
    "                    break\n",
    "                \n",
    "                # Click page button using JavaScript\n",
    "                print(f\"   Clicking page {page}...\", end=\" \", flush=True)\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", page_btn)\n",
    "                time.sleep(1)\n",
    "                driver.execute_script(\"arguments[0].click();\", page_btn)\n",
    "                time.sleep(8)  # Wait for AJAX page load\n",
    "                \n",
    "                # Scroll to load lazy-loaded content\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(2)\n",
    "                \n",
    "                # Extract events from this page\n",
    "                page_events = extract_event_urls_func(driver.page_source, category_name)\n",
    "                \n",
    "                if page_events:\n",
    "                    # Get set of event IDs on this page\n",
    "                    current_ids = set(e['id'] for e in page_events)\n",
    "                    \n",
    "                    # Check if events are different from previous page\n",
    "                    if current_ids == previous_ids:\n",
    "                        print(f\"Same events as page {page-1} - reached end\")\n",
    "                        break\n",
    "                    \n",
    "                    new_count = len(current_ids - previous_ids)\n",
    "                    print(f\"Found {len(page_events)} events ({new_count} new)\")\n",
    "                    all_events.extend(page_events)\n",
    "                    previous_ids = current_ids\n",
    "                else:\n",
    "                    print(f\"No events found - stopping pagination\")\n",
    "                    break\n",
    "                \n",
    "                page += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Pagination error: {e}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"   ‚úÖ Total events: {len(all_events)}\")\n",
    "        return all_events\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error in pagination: {e}\")\n",
    "        return all_events\n",
    "\n",
    "\n",
    "print(\"‚úÖ Pagination function defined - ready to use\")\n",
    "print(\"   Use: events = paginate_category(driver, category_name, extract_func)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845b3498",
   "metadata": {},
   "source": [
    "## ü§ñ Selenium Browser Setup\n",
    "\n",
    "Configure and initialize the Selenium WebDriver with anti-detection measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aad5b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ JonnusWebDriver class defined\n"
     ]
    }
   ],
   "source": [
    "class JonnusWebDriver:\n",
    "    \"\"\"Manages Selenium WebDriver with anti-detection measures\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        print(\"ü§ñ Initializing Joinnus WebDriver\")\n",
    "    \n",
    "    def setup_driver(self):\n",
    "        \"\"\"Setup Chrome WebDriver with anti-detection options\"\"\"\n",
    "        try:\n",
    "            chrome_options = Options()\n",
    "            \n",
    "            # Anti-detection measures\n",
    "            if SCRAPING_CONFIG['browser']['headless']:\n",
    "                chrome_options.add_argument(\"--headless\")\n",
    "            \n",
    "            chrome_options.add_argument(\"--no-sandbox\")\n",
    "            chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            \n",
    "            if SCRAPING_CONFIG['browser']['disable_gpu']:\n",
    "                chrome_options.add_argument(\"--disable-gpu\")\n",
    "            \n",
    "            chrome_options.add_argument(f\"--window-size={SCRAPING_CONFIG['browser']['window_size']}\")\n",
    "            chrome_options.add_argument(f\"user-agent={JOINNUS_CONFIG['headers']['User-Agent']}\")\n",
    "            chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "            chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "            chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "            \n",
    "            # Disable image loading for faster scraping\n",
    "            if not SCRAPING_CONFIG['browser']['disable_images']:\n",
    "                prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "                chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "            \n",
    "            # Initialize driver\n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "            \n",
    "            print(\"‚úÖ Chrome WebDriver initialized successfully\")\n",
    "            return self.driver\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error initializing WebDriver: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def quit(self):\n",
    "        \"\"\"Close the WebDriver\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            print(\"‚úÖ WebDriver closed\")\n",
    "    \n",
    "    def wait_for_element(self, selector, by=By.CSS_SELECTOR, timeout=None):\n",
    "        \"\"\"Wait for element to be present\"\"\"\n",
    "        timeout = timeout or SCRAPING_CONFIG['timing']['element_wait']\n",
    "        try:\n",
    "            WebDriverWait(self.driver, timeout).until(\n",
    "                EC.presence_of_element_located((by, selector))\n",
    "            )\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def scroll_to_bottom(self):\n",
    "        \"\"\"Scroll to bottom of page to trigger lazy loading\"\"\"\n",
    "        try:\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "print(\"‚úÖ JonnusWebDriver class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6977e9",
   "metadata": {},
   "source": [
    "## üîç Data Extraction Methods\n",
    "\n",
    "Define methods to extract event information from Joinnus pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e10c2451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ JonnusEventExtractor class defined\n",
      "‚úÖ Extraction and processing functions defined\n"
     ]
    }
   ],
   "source": [
    "class JonnusEventExtractor:\n",
    "    \"\"\"Extract event data from Joinnus pages\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        print(\"üîç Joinnus Event Extractor initialized\")\n",
    "    \n",
    "    def extract_event_urls_from_listing(self, html_content, category):\n",
    "        \"\"\"Extract event URLs and IDs from listing page using aria-label selector\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            events = []\n",
    "            \n",
    "            # Find all links with aria-label=\"Ver detalle del evento\"\n",
    "            event_links = soup.find_all('a', {'aria-label': 'Ver detalle del evento'})\n",
    "            \n",
    "            for link in event_links:\n",
    "                try:\n",
    "                    href = link.get('href')\n",
    "                    if not href:\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract ID from URL (last number in the URL)\n",
    "                    # URL format: .../event-name-12345\n",
    "                    match = re.search(r'-(\\d+)$', href)\n",
    "                    if match:\n",
    "                        event_id = match.group(1)\n",
    "                    else:\n",
    "                        # Try alternate format or use last part\n",
    "                        parts = href.strip('/').split('-')\n",
    "                        event_id = parts[-1] if parts[-1].isdigit() else None\n",
    "                    \n",
    "                    if not event_id:\n",
    "                        continue\n",
    "                    \n",
    "                    # Convert relative URLs to absolute\n",
    "                    if href.startswith('/'):\n",
    "                        href = JOINNUS_CONFIG['base_domain'] + href\n",
    "                    elif not href.startswith('http'):\n",
    "                        href = JOINNUS_CONFIG['base_domain'] + '/' + href\n",
    "                    \n",
    "                    event_data = {\n",
    "                        'id': event_id,\n",
    "                        'url': href,\n",
    "                        'category': category\n",
    "                    }\n",
    "                    events.append(event_data)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error extracting event from link: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"‚úÖ Found {len(events)} event URLs on page\")\n",
    "            return events\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error extracting event links: {e}\")\n",
    "            return []\n",
    "\n",
    "print(\"‚úÖ JonnusEventExtractor class defined\")\n",
    "\n",
    "\n",
    "def extract_and_store_events():\n",
    "    \"\"\"\n",
    "    Step 1: Extract events from all Joinnus categories and store in data/ directory\n",
    "    \"\"\"\n",
    "    print(\"üöÄ STEP 1: EXTRACTING EVENTS FROM ALL CATEGORIES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    web_driver_manager = None\n",
    "    all_events = []\n",
    "    \n",
    "    try:\n",
    "        # Load categories from CSV\n",
    "        urls_df = pd.read_csv(URLS_CONFIG['csv_file'])\n",
    "        urls_df.columns = urls_df.columns.str.strip()\n",
    "        categories = urls_df.to_dict('records')\n",
    "        \n",
    "        print(f\"üìÇ Found {len(categories)} categories to process\\n\")\n",
    "        \n",
    "        # Initialize driver\n",
    "        web_driver_manager = JonnusWebDriver()\n",
    "        driver = web_driver_manager.setup_driver()\n",
    "        \n",
    "        # Process each category\n",
    "        for idx, category_data in enumerate(categories, 1):\n",
    "            category_url = category_data['url'].strip()\n",
    "            category_name = category_data['category'].strip()\n",
    "            \n",
    "            print(f\"[{idx}/{len(categories)}] üìç {category_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Navigate to category\n",
    "                driver.get(category_url)\n",
    "                time.sleep(5)\n",
    "                web_driver_manager.scroll_to_bottom()\n",
    "                \n",
    "                # Extract events using the pagination function\n",
    "                extractor = JonnusEventExtractor()\n",
    "                category_events = paginate_category(driver, category_name, extractor.extract_event_urls_from_listing)\n",
    "                \n",
    "                all_events.extend(category_events)\n",
    "                print(f\"   ‚úÖ Total from this category: {len(category_events)}\\n\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error: {str(e)}\\n\")\n",
    "                continue\n",
    "        \n",
    "        # Save extracted events\n",
    "        if all_events:\n",
    "            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            \n",
    "            # Save as JSON\n",
    "            json_file = DATA_DIR / f\"events_{timestamp}.json\"\n",
    "            with open(json_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(all_events, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"‚úÖ Saved JSON: {json_file.name} ({len(all_events)} events)\")\n",
    "            \n",
    "            # Save as CSV\n",
    "            csv_file = DATA_DIR / f\"events_{timestamp}.csv\"\n",
    "            df = pd.DataFrame(all_events)\n",
    "            df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "            print(f\"‚úÖ Saved CSV: {csv_file.name}\")\n",
    "            \n",
    "            print(f\"\\nüìä EXTRACTION SUMMARY\")\n",
    "            print(\"=\" * 70)\n",
    "            print(f\"   Total events extracted: {len(all_events)}\")\n",
    "            print(f\"   Categories processed: {len(categories)}\")\n",
    "            print(f\"   Files saved to: {DATA_DIR}\")\n",
    "            \n",
    "            return all_events, json_file, csv_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Extraction failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [], None, None\n",
    "    \n",
    "    finally:\n",
    "        if web_driver_manager:\n",
    "            web_driver_manager.quit()\n",
    "            print(\"\\nüåê Browser closed\")\n",
    "\n",
    "\n",
    "def process_and_analyze_events(csv_file):\n",
    "    \"\"\"\n",
    "    Step 2: Read CSV and extract important data to JSON format\n",
    "    \"\"\"\n",
    "    print(\"\\nüîÑ STEP 2: PROCESSING AND ANALYZING EVENTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(csv_file)\n",
    "        print(f\"üìñ Read {len(df)} events from CSV\\n\")\n",
    "        \n",
    "        # Extract important data\n",
    "        analysis = {\n",
    "            'extraction_date': datetime.datetime.now().isoformat(),\n",
    "            'total_events': len(df),\n",
    "            'total_categories': df['category'].nunique() if 'category' in df.columns else 0,\n",
    "            'categories': {},\n",
    "            'sample_events': df.head(10).to_dict('records')\n",
    "        }\n",
    "        \n",
    "        # Analyze by category\n",
    "        if 'category' in df.columns:\n",
    "            category_counts = df['category'].value_counts().to_dict()\n",
    "            for cat, count in sorted(category_counts.items()):\n",
    "                analysis['categories'][cat] = {\n",
    "                    'event_count': int(count),\n",
    "                    'percentage': round((count / len(df)) * 100, 2)\n",
    "                }\n",
    "            \n",
    "            print(\"üìä Events by Category:\")\n",
    "            for cat, info in analysis['categories'].items():\n",
    "                print(f\"   {cat}: {info['event_count']} events ({info['percentage']}%)\")\n",
    "        \n",
    "        # Save analysis to JSON\n",
    "        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        analysis_file = DATA_DIR / f\"analysis_{timestamp}.json\"\n",
    "        \n",
    "        with open(analysis_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(analysis, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Saved analysis: {analysis_file.name}\")\n",
    "        \n",
    "        print(f\"\\nüìà ANALYSIS SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"   Total events: {analysis['total_events']}\")\n",
    "        print(f\"   Total categories: {analysis['total_categories']}\")\n",
    "        print(f\"   Files saved to: {DATA_DIR}\")\n",
    "        \n",
    "        return analysis, analysis_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Processing failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "\n",
    "print(\"‚úÖ Extraction and processing functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a92801",
   "metadata": {},
   "source": [
    "## ‚ñ∂Ô∏è Run Complete Workflow\n",
    "\n",
    "Execute the full extraction ‚Üí storage ‚Üí analysis pipeline in one go.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "044b4d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:12:23,182 - WDM - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ JOINNUS EVENTS EXTRACTION & ANALYSIS PIPELINE\n",
      "======================================================================\n",
      "This process will:\n",
      "  1Ô∏è‚É£ Extract events from all Joinnus categories\n",
      "  2Ô∏è‚É£ Store results in 'data/' folder (JSON + CSV)\n",
      "  3Ô∏è‚É£ Analyze results and save stats to JSON\n",
      "======================================================================\n",
      "\n",
      "üöÄ STEP 1: EXTRACTING EVENTS FROM ALL CATEGORIES\n",
      "======================================================================\n",
      "üìÇ Found 19 categories to process\n",
      "\n",
      "ü§ñ Initializing Joinnus WebDriver\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:12:23,853 - WDM - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-11-02 13:12:24,200 - WDM - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-11-02 13:12:24,200 - WDM - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-11-02 13:12:24,433 - WDM - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-11-02 13:12:24,433 - WDM - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-11-02 13:12:25,087 - WDM - INFO - WebDriver version 141.0.7390.122 selected\n",
      "2025-11-02 13:12:25,091 - WDM - INFO - Modern chrome version https://storage.googleapis.com/chrome-for-testing-public/141.0.7390.122/win32/chromedriver-win32.zip\n",
      "2025-11-02 13:12:25,092 - WDM - INFO - About to download new driver from https://storage.googleapis.com/chrome-for-testing-public/141.0.7390.122/win32/chromedriver-win32.zip\n",
      "2025-11-02 13:12:25,087 - WDM - INFO - WebDriver version 141.0.7390.122 selected\n",
      "2025-11-02 13:12:25,091 - WDM - INFO - Modern chrome version https://storage.googleapis.com/chrome-for-testing-public/141.0.7390.122/win32/chromedriver-win32.zip\n",
      "2025-11-02 13:12:25,092 - WDM - INFO - About to download new driver from https://storage.googleapis.com/chrome-for-testing-public/141.0.7390.122/win32/chromedriver-win32.zip\n",
      "2025-11-02 13:12:25,416 - WDM - INFO - Driver downloading response is 200\n",
      "2025-11-02 13:12:25,416 - WDM - INFO - Driver downloading response is 200\n",
      "2025-11-02 13:12:25,755 - WDM - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-11-02 13:12:25,755 - WDM - INFO - Get LATEST chromedriver version for google-chrome\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê Browser closed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Step 1: Extract and store events\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m events, events_json_file, events_csv_file = \u001b[43mextract_and_store_events\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Step 2: Process and analyze (only if extraction succeeded)\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events_csv_file:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mextract_and_store_events\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Initialize driver\u001b[39;00m\n\u001b[32m     82\u001b[39m web_driver_manager = JonnusWebDriver()\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m driver = \u001b[43mweb_driver_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup_driver\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Process each category\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, category_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(categories, \u001b[32m1\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mJonnusWebDriver.setup_driver\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     32\u001b[39m     chrome_options.add_experimental_option(\u001b[33m\"\u001b[39m\u001b[33mprefs\u001b[39m\u001b[33m\"\u001b[39m, prefs)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Initialize driver\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m service = Service(\u001b[43mChromeDriverManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minstall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28mself\u001b[39m.driver = webdriver.Chrome(service=service, options=chrome_options)\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Chrome WebDriver initialized successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Scrapping\\.venv\\Lib\\site-packages\\webdriver_manager\\chrome.py:40\u001b[39m, in \u001b[36mChromeDriverManager.install\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minstall\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     driver_path = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_driver_binary_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     os.chmod(driver_path, \u001b[32m0o755\u001b[39m)\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m driver_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Scrapping\\.venv\\Lib\\site-packages\\webdriver_manager\\core\\manager.py:41\u001b[39m, in \u001b[36mDriverManager._get_driver_binary_path\u001b[39m\u001b[34m(self, driver)\u001b[39m\n\u001b[32m     39\u001b[39m os_type = \u001b[38;5;28mself\u001b[39m.get_os_type()\n\u001b[32m     40\u001b[39m file = \u001b[38;5;28mself\u001b[39m._download_manager.download_file(driver.get_driver_download_url(os_type))\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m binary_path = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cache_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_file_to_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m binary_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Scrapping\\.venv\\Lib\\site-packages\\webdriver_manager\\core\\driver_cache.py:54\u001b[39m, in \u001b[36mDriverCacheManager.save_file_to_cache\u001b[39m\u001b[34m(self, driver, file)\u001b[39m\n\u001b[32m     52\u001b[39m path = \u001b[38;5;28mself\u001b[39m.__get_path(driver)\n\u001b[32m     53\u001b[39m archive = \u001b[38;5;28mself\u001b[39m.save_archive_file(file, path)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m files = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munpack_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m binary = \u001b[38;5;28mself\u001b[39m.__get_binary(files, driver.get_name())\n\u001b[32m     56\u001b[39m binary_path = os.path.join(path, binary)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Scrapping\\.venv\\Lib\\site-packages\\webdriver_manager\\core\\driver_cache.py:49\u001b[39m, in \u001b[36mDriverCacheManager.unpack_archive\u001b[39m\u001b[34m(self, archive, path)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munpack_archive\u001b[39m(\u001b[38;5;28mself\u001b[39m, archive, path):\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_file_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43munpack_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Scrapping\\.venv\\Lib\\site-packages\\webdriver_manager\\core\\file_manager.py:57\u001b[39m, in \u001b[36mFileManager.unpack_archive\u001b[39m\u001b[34m(self, archive_file, target_dir)\u001b[39m\n\u001b[32m     55\u001b[39m file_path = archive_file.file_path\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file_path.endswith(\u001b[33m\"\u001b[39m\u001b[33m.zip\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__extract_zip\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchive_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m file_path.endswith(\u001b[33m\"\u001b[39m\u001b[33m.tar.gz\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__extract_tar_file(archive_file, target_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Scrapping\\.venv\\Lib\\site-packages\\webdriver_manager\\core\\file_manager.py:63\u001b[39m, in \u001b[36mFileManager.__extract_zip\u001b[39m\u001b[34m(self, archive_file, to_directory)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__extract_zip\u001b[39m(\u001b[38;5;28mself\u001b[39m, archive_file, to_directory):\n\u001b[32m     62\u001b[39m     zip_class = (LinuxZipFileWithPermissions \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._os_system_manager.get_os_name() == \u001b[33m\"\u001b[39m\u001b[33mlinux\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m zipfile.ZipFile)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     archive = \u001b[43mzip_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchive_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     65\u001b[39m         archive.extractall(to_directory)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\zipfile\\__init__.py:1367\u001b[39m, in \u001b[36mZipFile.__init__\u001b[39m\u001b[34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[39m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1367\u001b[39m         \u001b[38;5;28mself\u001b[39m.fp = \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1368\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1369\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ‚ñ∂Ô∏è RUN THE COMPLETE WORKFLOW\n",
    "# This will:\n",
    "# 1. Extract events from all 19 categories (15-30 minutes)\n",
    "# 2. Save to data/events_TIMESTAMP.json and .csv\n",
    "# 3. Analyze and save stats to data/analysis_TIMESTAMP.json\n",
    "\n",
    "print(\"üéØ JOINNUS EVENTS EXTRACTION & ANALYSIS PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"This process will:\")\n",
    "print(\"  1Ô∏è‚É£ Extract events from all Joinnus categories\")\n",
    "print(\"  2Ô∏è‚É£ Store results in 'data/' folder (JSON + CSV)\")\n",
    "print(\"  3Ô∏è‚É£ Analyze results and save stats to JSON\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Step 1: Extract and store events\n",
    "events, events_json_file, events_csv_file = extract_and_store_events()\n",
    "\n",
    "# Step 2: Process and analyze (only if extraction succeeded)\n",
    "if events_csv_file:\n",
    "    analysis, analysis_file = process_and_analyze_events(events_csv_file)\n",
    "    \n",
    "    print(f\"\\nüéâ WORKFLOW COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"‚úÖ All files saved to: {DATA_DIR}\")\n",
    "    print(f\"   üìÑ Events JSON: events_*.json\")\n",
    "    print(f\"   üìä Events CSV: events_*.csv\")\n",
    "    print(f\"   üìà Analysis: analysis_*.json\")\n",
    "    print(f\"\\nüìä Quick Stats:\")\n",
    "    print(f\"   Total events: {len(events)}\")\n",
    "    print(f\"   Total categories: {analysis['total_categories']}\")\n",
    "    print(f\"   Top categories:\")\n",
    "    sorted_cats = sorted(analysis['categories'].items(), key=lambda x: x[1]['event_count'], reverse=True)\n",
    "    for cat, info in sorted_cats[:5]:\n",
    "        print(f\"      ‚Ä¢ {cat}: {info['event_count']} events\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Workflow stopped - extraction failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c790123",
   "metadata": {},
   "source": [
    "## üìã Step 2: Extract Comprehensive Event Data\n",
    "\n",
    "Extract detailed information from each event page (title, description, images, pricing, tags, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdc9367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Event detail extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_from_json_ld(soup):\n",
    "    \"\"\"Extract structured data from JSON-LD schema\"\"\"\n",
    "    try:\n",
    "        scripts = soup.find_all('script', {'type': 'application/ld+json'})\n",
    "        for script in scripts:\n",
    "            data = json.loads(script.string)\n",
    "            if isinstance(data, dict) and data.get('@type') == 'Event':\n",
    "                return data\n",
    "    except:\n",
    "        pass\n",
    "    return {}\n",
    "\n",
    "\n",
    "def extract_event_details(driver, url, event_id, category):\n",
    "    \"\"\"\n",
    "    Extract comprehensive event data from a Joinnus event page.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Extracted event data with all fields\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Scroll to load lazy-loaded content\n",
    "        for _ in range(3):\n",
    "            driver.execute_script(\"window.scrollBy(0, window.innerHeight);\")\n",
    "            time.sleep(0.3)\n",
    "        \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        json_ld = extract_from_json_ld(soup)\n",
    "        \n",
    "        data = {\n",
    "            'event_id': event_id,\n",
    "            'url': url,\n",
    "            'category': category,\n",
    "            'title': None,\n",
    "            'description': None,\n",
    "            'city': None,\n",
    "            'location_venue': None,\n",
    "            'address': None,\n",
    "            'rating': None,\n",
    "            'event_type': None,\n",
    "            'price_min': None,\n",
    "            'price_currency': None,\n",
    "            'tags': [],\n",
    "            'images': [],\n",
    "            'start_date': None,\n",
    "            'end_date': None,\n",
    "            'times': [],\n",
    "            'extracted_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Extract title\n",
    "        h2_tag = soup.find('h2', class_='text-xl')\n",
    "        if h2_tag:\n",
    "            data['title'] = h2_tag.get_text(strip=True)\n",
    "        elif json_ld.get('name'):\n",
    "            data['title'] = json_ld['name']\n",
    "        else:\n",
    "            og_title = soup.find('meta', {'property': 'og:title'})\n",
    "            if og_title:\n",
    "                data['title'] = og_title.get('content', '').split('|')[0].strip()\n",
    "        \n",
    "        # Extract description (strip HTML tags and decode HTML entities)\n",
    "        og_desc = soup.find('meta', {'property': 'og:description'})\n",
    "        if og_desc:\n",
    "            raw_desc = og_desc.get('content')\n",
    "        elif json_ld.get('description'):\n",
    "            raw_desc = json_ld['description']\n",
    "        else:\n",
    "            raw_desc = None\n",
    "\n",
    "        if raw_desc:\n",
    "            try:\n",
    "                cleaned = BeautifulSoup(raw_desc, 'html.parser').get_text(separator=' ', strip=True)\n",
    "                cleaned = unescape(cleaned)\n",
    "                data['description'] = cleaned\n",
    "            except Exception:\n",
    "                data['description'] = raw_desc\n",
    "        \n",
    "        # Extract city and location from spans\n",
    "        spans = soup.find_all('span', class_='h-full')\n",
    "        span_texts = [s.get_text(strip=True) for s in spans]\n",
    "        if len(span_texts) > 1:\n",
    "            data['city'] = span_texts[1]\n",
    "        \n",
    "        # Extract location venue (Real Plaza Angamos, MALI, etc.)\n",
    "        for i, span in enumerate(spans):\n",
    "            prev_text = span.find_previous(string=True)\n",
    "            if prev_text and 'Ubicaci√≥n' in prev_text:\n",
    "                next_h_full = span.find_next('span', class_='h-full')\n",
    "                if next_h_full:\n",
    "                    data['location_venue'] = next_h_full.get_text(strip=True)\n",
    "                break\n",
    "        \n",
    "        # Extract address from JSON-LD\n",
    "        if json_ld.get('location', {}).get('address', {}).get('streetAddress'):\n",
    "            data['address'] = json_ld['location']['address']['streetAddress']\n",
    "        \n",
    "        # Extract rating\n",
    "        rating_span = soup.find('span', class_=['base-rating'])\n",
    "        if rating_span:\n",
    "            data['rating'] = rating_span.get_text(strip=True)\n",
    "        \n",
    "        # Extract price info from JSON-LD\n",
    "        if json_ld.get('offers', {}).get('lowPrice') is not None:\n",
    "            data['price_min'] = json_ld['offers']['lowPrice']\n",
    "            data['price_currency'] = json_ld['offers'].get('priceCurrency')\n",
    "        \n",
    "        # Extract event type/audience\n",
    "        audience_span = soup.find('span', class_=re.compile('text-\\\\[0.625rem\\\\]'))\n",
    "        if audience_span:\n",
    "            text = audience_span.get_text(strip=True)\n",
    "            if 'Apto' in text or 'General' in text:\n",
    "                data['event_type'] = text\n",
    "        \n",
    "        # Extract tags/categories from all spans\n",
    "        all_h_full_spans = soup.find_all('span', class_='h-full')\n",
    "        for span in all_h_full_spans:\n",
    "            text = span.get_text(strip=True)\n",
    "            if text and text not in ['Lima', data.get('city', ''), 'Descubrir']:\n",
    "                if 2 < len(text) < 40 and len(text.split()) <= 3:\n",
    "                    if text not in data['tags']:\n",
    "                        data['tags'].append(text)\n",
    "        \n",
    "        data['tags'] = list(set(data['tags']))[:20]\n",
    "        \n",
    "        # Extract images: prefer S3-hosted event images on classic routes and avoid generic site assets\n",
    "        images = []\n",
    "        img_tags = soup.find_all('img')\n",
    "        candidate_srcs = []\n",
    "        for img in img_tags:\n",
    "            src = img.get('src') or img.get('data-src') or img.get('data-lazy-src') or img.get('data-original')\n",
    "            if not src:\n",
    "                continue\n",
    "            # Normalize protocol-relative and relative URLs\n",
    "            if src.startswith('//'):\n",
    "                src = 'https:' + src\n",
    "            if src.startswith('/') and not src.startswith('//') and 'http' not in src:\n",
    "                base = JOINNUS_CONFIG.get('base_domain','').rstrip('/')\n",
    "                src = base + src\n",
    "            candidate_srcs.append(src)\n",
    "        # Deduplicate preserving order\n",
    "        seen = set()\n",
    "        candidate_srcs = [x for x in candidate_srcs if not (x in seen or seen.add(x))]\n",
    "        \n",
    "        def is_generic_asset(s):\n",
    "            low = s.lower()\n",
    "            if 'maps-preview.png' in low:\n",
    "                return True\n",
    "            if '/profile/' in low:\n",
    "                return True\n",
    "            if '/files/' in low:\n",
    "                return True\n",
    "            if '/libro-de-reclamaciones' in low:\n",
    "                return True\n",
    "            if '/icons/' in low or '/icon-' in low:\n",
    "                return True\n",
    "            if 'placeholder' in low or 'avatar' in low:\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        # Identify S3 and CDN candidates\n",
    "        s3_images = [s for s in candidate_srcs if 's3.' in s and 'joinnus.com' in s and not is_generic_asset(s)]\n",
    "        cdn_images = [s for s in candidate_srcs if 'cdn.joinnus.com' in s and not is_generic_asset(s)]\n",
    "        \n",
    "        # Determine if this is a classic route (only apply S3-priority for classic pages)\n",
    "        is_classic = False\n",
    "        classic_domain = JOINNUS_CONFIG.get('classic_domain','')\n",
    "        try:\n",
    "            if classic_domain and classic_domain.replace('https://','').replace('http://','') in url:\n",
    "                is_classic = True\n",
    "            elif 'classic.joinnus.com' in url:\n",
    "                is_classic = True\n",
    "        except Exception:\n",
    "            is_classic = False\n",
    "        \n",
    "        # Ordering: for classic pages prefer S3 images first, otherwise prefer CDN images first\n",
    "        selected = []\n",
    "        if is_classic:\n",
    "            for s in s3_images:\n",
    "                if s not in selected:\n",
    "                    selected.append(s)\n",
    "            for s in cdn_images:\n",
    "                if s not in selected:\n",
    "                    selected.append(s)\n",
    "        else:\n",
    "            for s in cdn_images:\n",
    "                if s not in selected:\n",
    "                    selected.append(s)\n",
    "            for s in s3_images:\n",
    "                if s not in selected:\n",
    "                    selected.append(s)\n",
    "        \n",
    "        # Fallback: include any non-data images not matching generic patterns\n",
    "        for s in candidate_srcs:\n",
    "            if s not in selected and not s.startswith('data:') and not is_generic_asset(s):\n",
    "                selected.append(s)\n",
    "        \n",
    "        data['images'] = selected[:5]\n",
    "        \n",
    "        # Extract dates and times from JSON-LD\n",
    "        if json_ld.get('startDate'):\n",
    "            data['start_date'] = json_ld['startDate']\n",
    "        if json_ld.get('endDate'):\n",
    "            data['end_date'] = json_ld['endDate']\n",
    "        \n",
    "        date_time_ps = soup.find_all('p', class_=['flex', 'gap-1'])\n",
    "        for p in date_time_ps:\n",
    "            text = p.get_text(strip=True)\n",
    "            if re.search(r'\\\\d{1,2}:\\\\d{2}', text):\n",
    "                if text not in data['times']:\n",
    "                    data['times'].append(text)\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting details from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"‚úÖ Event detail extraction functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ca1906fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Comprehensive extraction function defined\n",
      "   Use: all_events, json_file, csv_file = extract_detailed_event_data(csv_path, check_mongodb=True)\n"
     ]
    }
   ],
   "source": [
    "def extract_detailed_event_data(csv_file_path, check_mongodb=True):\n",
    "    \"\"\"\n",
    "    Extract comprehensive event data from all events in CSV file.\n",
    "    - Skips already extracted events by checking data/events/ folder\n",
    "    - Also checks MongoDB if available\n",
    "    - Saves each event immediately to data/events/ folder as individual JSON files\n",
    "    - Creates combined JSON at end in data/ directory\n",
    "    \n",
    "    Args:\n",
    "        csv_file_path: Path to CSV with event URLs\n",
    "        check_mongodb: If True, also check MongoDB for existing event IDs\n",
    "        \n",
    "    Returns:\n",
    "        List of extracted event data dictionaries\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"COMPREHENSIVE EVENT DATA EXTRACTION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Read CSV file\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        total_events = len(df)\n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(\"CSV ANALYSIS (BEFORE EXTRACTION)\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"üìñ Loaded {total_events} events from CSV\")\n",
    "        print(f\"üìÇ Processing started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        \n",
    "        # Normalize column names (strip whitespace)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        print(f\"   Available columns: {list(df.columns)}\\n\")\n",
    "        \n",
    "        # Show CSV stats BEFORE extraction\n",
    "        print(f\"Total rows in CSV: {total_events}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Determine which columns to use (handle different naming variations)\n",
    "        id_col = None\n",
    "        url_col = None\n",
    "        category_col = None\n",
    "        \n",
    "        # Find ID column\n",
    "        for col in ['id', 'event_id', 'ID', 'Event ID']:\n",
    "            if col in df.columns:\n",
    "                id_col = col\n",
    "                break\n",
    "        \n",
    "        # Find URL column\n",
    "        for col in ['url', 'URL', 'event_url', 'link']:\n",
    "            if col in df.columns:\n",
    "                url_col = col\n",
    "                break\n",
    "        \n",
    "        # Find category column\n",
    "        for col in ['category', 'Category', 'event_category']:\n",
    "            if col in df.columns:\n",
    "                category_col = col\n",
    "                break\n",
    "        \n",
    "        if not id_col or not url_col:\n",
    "            print(f\"‚ùå Error: CSV missing required columns\")\n",
    "            print(f\"   Need: 'id' (or 'event_id') and 'url'\")\n",
    "            print(f\"   Found columns: {list(df.columns)}\")\n",
    "            return [], None, None\n",
    "        \n",
    "        print(f\"‚úì Using columns: id='{id_col}', url='{url_col}', category='{category_col}'\\n\")\n",
    "        \n",
    "        # Create events directory for individual files\n",
    "        events_dir = DATA_DIR / 'events'\n",
    "        events_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Get list of already extracted event IDs from local files\n",
    "        already_extracted = set()\n",
    "        for event_file in events_dir.glob('event_*.json'):\n",
    "            event_id = event_file.stem.replace('event_', '')\n",
    "            already_extracted.add(event_id)\n",
    "        \n",
    "        # Also check MongoDB if requested\n",
    "        mongodb_ids = set()\n",
    "        if check_mongodb:\n",
    "            try:\n",
    "                mongo_store = MongoDBEventStore(MONGODB_CONFIG)\n",
    "                if mongo_store.connect():\n",
    "                    mongodb_ids = mongo_store.get_existing_ids()\n",
    "                    mongo_store.close()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        \n",
    "        # Combine both sets of IDs to skip\n",
    "        skip_ids = already_extracted | mongodb_ids\n",
    "        total_skip = len(skip_ids)\n",
    "        to_extract = total_events - total_skip\n",
    "        \n",
    "        # Display skip analysis before extraction\n",
    "        print(f\"\\nüìä SKIP ANALYSIS:\")\n",
    "        print(f\"   Already in data/events/ folder: {len(already_extracted)}\")\n",
    "        print(f\"   Already in MongoDB: {len(mongodb_ids)}\")\n",
    "        print(f\"   Total to skip: {total_skip}\")\n",
    "        print(f\"   New events to extract: {to_extract}\")\n",
    "        \n",
    "        if total_events > 0:\n",
    "            skip_pct = (total_skip / total_events) * 100\n",
    "            extract_pct = (to_extract / total_events) * 100\n",
    "            print(f\"\\n   Breakdown: {skip_pct:.1f}% skip | {extract_pct:.1f}% extract\")\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(\"STARTING EXTRACTION\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "        \n",
    "        # Initialize WebDriver\n",
    "        web_driver = JonnusWebDriver()\n",
    "        driver = web_driver.setup_driver()\n",
    "        \n",
    "        all_events = []\n",
    "        failed_events = []\n",
    "        skipped_count = 0\n",
    "        duplicated_count = 0\n",
    "\n",
    "        repeat = set()\n",
    "        try:\n",
    "            for idx, row in df.iterrows():\n",
    "                url = str(row[url_col])\n",
    "                event_id = str(row[id_col])\n",
    "                category = str(row[category_col]) if category_col else 'Unknown'\n",
    "\n",
    "                if event_id in repeat:\n",
    "                    duplicated_count += 1\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                repeat.add(event_id)\n",
    "                \n",
    "                # Skip if already extracted (local) or in database (MongoDB)\n",
    "                if event_id in skip_ids:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Progress indicator (adjusted for skipped events)\n",
    "                processed_count = idx + 1 - skipped_count\n",
    "                total_to_process = total_events - len(skip_ids)\n",
    "                progress_pct = (processed_count / total_to_process * 100) if total_to_process > 0 else 0\n",
    "                \n",
    "                print(f\"[{processed_count:4d}/{total_to_process}] ({progress_pct:5.1f}%) Event {event_id}...\", end='', flush=True)\n",
    "                \n",
    "                # Extract detailed data\n",
    "                event_data = extract_event_details(driver, url, event_id, category)\n",
    "                \n",
    "                if event_data:\n",
    "                    all_events.append(event_data)\n",
    "                    \n",
    "                    # Save individual event immediately to events/ folder\n",
    "                    event_file = events_dir / f\"event_{event_id}.json\"\n",
    "                    with open(event_file, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(event_data, f, ensure_ascii=False, indent=2)\n",
    "                    \n",
    "                    print(f\" ‚úì {event_data['title'][:40] if event_data['title'] else 'Unknown'}\")\n",
    "                else:\n",
    "                    failed_events.append({'id': event_id, 'url': url})\n",
    "                    print(f\" ‚ö†Ô∏è Failed\")\n",
    "                \n",
    "                # Restart browser every 50 events for stability\n",
    "                if (processed_count) % 50 == 0 and processed_count > 0:\n",
    "                    print(f\"\\n  üîÑ Restarting browser for stability...\\n\")\n",
    "                    driver.quit()\n",
    "                    driver = web_driver.setup_driver()\n",
    "            \n",
    "        finally:\n",
    "            driver.quit()\n",
    "            print(\"\\n‚úì Browser closed\")\n",
    "        \n",
    "        # Load already extracted events for combined file\n",
    "        print(f\"\\nüîÑ Loading previously extracted events...\")\n",
    "        for event_id in already_extracted:\n",
    "            event_file = events_dir / f\"event_{event_id}.json\"\n",
    "            if event_file.exists():\n",
    "                try:\n",
    "                    with open(event_file, 'r', encoding='utf-8') as f:\n",
    "                        event_data = json.load(f)\n",
    "                        all_events.append(event_data)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error loading {event_file}: {e}\")\n",
    "        \n",
    "        all_events.sort(key=lambda x: int(x['event_id']))\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(\"EXTRACTION SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Total processed (new): {len(all_events) - len(already_extracted)}/{total_events - len(skip_ids)}\")\n",
    "        print(f\"Previously extracted (local): {len(already_extracted)}\")\n",
    "        print(f\"Already in MongoDB: {len(mongodb_ids)}\")\n",
    "        print(f\"Total unique events: {len(all_events)}\")\n",
    "        print(f\"Duplicates in CSV skipped: {duplicated_count}\")\n",
    "        print(f\"Failed extractions: {len(failed_events)}\")\n",
    "        if total_events > 0:\n",
    "            success_rate = (len(all_events) / total_events * 100)\n",
    "            print(f\"Overall success rate: {success_rate:.1f}%\")\n",
    "        \n",
    "        print(f\"\\nüìÅ Individual Event Files Created:\")\n",
    "        print(f\"   Location: {events_dir.name}/ ({len(all_events)} event_*.json files)\")\n",
    "        \n",
    "        if failed_events:\n",
    "            failed_file = DATA_DIR / f'events_failed_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "            with open(failed_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(failed_events, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"‚ö†Ô∏è  Failed events: {failed_file.name} ({len(failed_events)} events)\")\n",
    "        \n",
    "        print(f\"\\nüí° Note: Run the combining cell to generate combined JSON/CSV files\")\n",
    "        \n",
    "        return all_events, None, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [], None, None\n",
    "\n",
    "\n",
    "print(\"‚úÖ Comprehensive extraction function defined\")\n",
    "print(\"   Use: all_events, json_file, csv_file = extract_detailed_event_data(csv_path, check_mongodb=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e7c66",
   "metadata": {},
   "source": [
    "## ‚ñ∂Ô∏è Execute Step 2: Comprehensive Extraction\n",
    "\n",
    "Run this to extract detailed data from all 603+ events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "27dddc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 14:43:58,222 - WDM - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Using CSV file: events_combined.csv\n",
      "üìÖ Created: 2025-10-24 14:29:12.327344\n",
      "\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE EVENT DATA EXTRACTION\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "CSV ANALYSIS (BEFORE EXTRACTION)\n",
      "================================================================================\n",
      "üìñ Loaded 518 events from CSV\n",
      "üìÇ Processing started at 2025-10-24 14:43:58\n",
      "\n",
      "   Available columns: ['event_id', 'title', 'category', 'city', 'location_venue', 'address', 'rating', 'price_min', 'price_currency', 'image_count', 'tag_count', 'time_slots', 'url']\n",
      "\n",
      "Total rows in CSV: 518\n",
      "Columns: ['event_id', 'title', 'category', 'city', 'location_venue', 'address', 'rating', 'price_min', 'price_currency', 'image_count', 'tag_count', 'time_slots', 'url']\n",
      "‚úì Using columns: id='event_id', url='url', category='category'\n",
      "\n",
      "üóÑÔ∏è Initializing MongoDB Event Store\n",
      "‚úÖ Connected to MongoDB\n",
      "   Database: recommendations-system\n",
      "   Collection: events\n",
      "üìä Found 0 existing events in MongoDB\n",
      "‚úÖ MongoDB connection closed\n",
      "\n",
      "üìä SKIP ANALYSIS:\n",
      "   Already in data/events/ folder: 518\n",
      "   Already in MongoDB: 0\n",
      "   Total to skip: 518\n",
      "   New events to extract: 0\n",
      "\n",
      "   Breakdown: 100.0% skip | 0.0% extract\n",
      "\n",
      "================================================================================\n",
      "STARTING EXTRACTION\n",
      "================================================================================\n",
      "\n",
      "ü§ñ Initializing Joinnus WebDriver\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 14:43:58,894 - WDM - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-10-24 14:43:59,108 - WDM - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-10-24 14:43:59,108 - WDM - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-10-24 14:43:59,322 - WDM - INFO - Driver [C:\\Users\\Singoe\\.wdm\\drivers\\chromedriver\\win64\\141.0.7390.122\\chromedriver-win32/chromedriver.exe] found in cache\n",
      "2025-10-24 14:43:59,322 - WDM - INFO - Driver [C:\\Users\\Singoe\\.wdm\\drivers\\chromedriver\\win64\\141.0.7390.122\\chromedriver-win32/chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chrome WebDriver initialized successfully\n",
      "\n",
      "‚úì Browser closed\n",
      "\n",
      "üîÑ Loading previously extracted events...\n",
      "\n",
      "================================================================================\n",
      "EXTRACTION SUMMARY\n",
      "================================================================================\n",
      "Total processed (new): 0/0\n",
      "Previously extracted (local): 518\n",
      "Already in MongoDB: 0\n",
      "Total unique events: 518\n",
      "Duplicates in CSV skipped: 0\n",
      "Failed extractions: 0\n",
      "Overall success rate: 100.0%\n",
      "\n",
      "üìÅ Individual Event Files Created:\n",
      "   Location: events/ (518 event_*.json files)\n",
      "\n",
      "üí° Note: Run the combining cell to generate combined JSON/CSV files\n",
      "\n",
      "üéâ EXTRACTION COMPLETE!\n",
      "   Total events extracted: 518\n",
      "   JSON file: N/A\n",
      "   CSV file: N/A\n",
      "\n",
      "‚úì Browser closed\n",
      "\n",
      "üîÑ Loading previously extracted events...\n",
      "\n",
      "================================================================================\n",
      "EXTRACTION SUMMARY\n",
      "================================================================================\n",
      "Total processed (new): 0/0\n",
      "Previously extracted (local): 518\n",
      "Already in MongoDB: 0\n",
      "Total unique events: 518\n",
      "Duplicates in CSV skipped: 0\n",
      "Failed extractions: 0\n",
      "Overall success rate: 100.0%\n",
      "\n",
      "üìÅ Individual Event Files Created:\n",
      "   Location: events/ (518 event_*.json files)\n",
      "\n",
      "üí° Note: Run the combining cell to generate combined JSON/CSV files\n",
      "\n",
      "üéâ EXTRACTION COMPLETE!\n",
      "   Total events extracted: 518\n",
      "   JSON file: N/A\n",
      "   CSV file: N/A\n"
     ]
    }
   ],
   "source": [
    "# Find the most recent events CSV file in data directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Look for the CSV created in previous extraction step\n",
    "csv_files = sorted(DATA_DIR.glob('events_*.csv'), key=os.path.getctime, reverse=True)\n",
    "\n",
    "if csv_files:\n",
    "    # Use the most recent CSV file\n",
    "    latest_csv = csv_files[0]\n",
    "    print(f\"üìÇ Using CSV file: {latest_csv.name}\")\n",
    "    print(f\"üìÖ Created: {datetime.fromtimestamp(latest_csv.stat().st_ctime)}\\n\")\n",
    "    \n",
    "    # Run comprehensive extraction (all analysis and skip logic is inside the function)\n",
    "    detailed_events, detailed_json, detailed_csv = extract_detailed_event_data(latest_csv)\n",
    "    \n",
    "    if detailed_events:\n",
    "        print(f\"\\nüéâ EXTRACTION COMPLETE!\")\n",
    "        print(f\"   Total events extracted: {len(detailed_events)}\")\n",
    "        print(f\"   JSON file: {detailed_json.name if detailed_json else 'N/A'}\")\n",
    "        print(f\"   CSV file: {detailed_csv.name if detailed_csv else 'N/A'}\")\n",
    "else:\n",
    "    print(\"‚ùå No events CSV file found in data directory\")\n",
    "    print(\"   Please run the initial extraction pipeline first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7a54878e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMBINING ALL EVENT JSON FILES\n",
      "================================================================================\n",
      "\n",
      "üìÅ Events directory: c:\\Scrapping\\joinnus\\notebook\\data\\events\n",
      "üìñ Found 518 individual event JSON files\n",
      "\n",
      "   Loaded 100/518 files...\n",
      "   Loaded 200/518 files...\n",
      "   Loaded 300/518 files...\n",
      "   Loaded 400/518 files...\n",
      "   Loaded 500/518 files...\n",
      "\n",
      "‚úÖ Loaded 518 total events\n",
      "\n",
      "üìã Fields in combined JSON: event_id, url, category, title, description, city, location_venue, address, rating, price_min, price_currency, tags, images, start_date, end_date, times, extracted_at\n",
      "\n",
      "‚úÖ Saved combined JSON: events_combined.json\n",
      "   Location: c:\\Scrapping\\joinnus\\notebook\\data\n",
      "   File size: 0.72 MB\n",
      "‚úÖ Saved combined CSV: events_combined.csv\n",
      "   Location: c:\\Scrapping\\joinnus\\notebook\\data\n",
      "\n",
      "================================================================================\n",
      "COMBINED DATA SUMMARY\n",
      "================================================================================\n",
      "Total events: 518\n",
      "\n",
      "Data Quality by Field:\n",
      "  title               :  518/518 (100.0%)\n",
      "  description         :  518/518 (100.0%)\n",
      "  city                :  331/518 ( 63.9%)\n",
      "  location_venue      :  176/518 ( 34.0%)\n",
      "  address             :  516/518 ( 99.6%)\n",
      "  price_min           :  483/518 ( 93.2%)\n",
      "  images              :  518/518 (100.0%)\n",
      "  times               :  333/518 ( 64.3%)\n",
      "  tags                :  333/518 ( 64.3%)\n",
      "  rating              :  333/518 ( 64.3%)\n",
      "  start_date          :  516/518 ( 99.6%)\n",
      "\n",
      "üìÇ Output files created:\n",
      "   JSON: events_combined.json\n",
      "   CSV:  events_combined.csv\n",
      "\n",
      "‚úÖ COMBINE COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "# Combine all individual event JSON files into a single master file\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime as dt\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMBINING ALL EVENT JSON FILES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Path to individual event files\n",
    "events_dir = DATA_DIR / 'events'\n",
    "all_events = []\n",
    "\n",
    "# Define important fields to keep in combined JSON\n",
    "IMPORTANT_FIELDS = [\n",
    "    'event_id', 'url', 'category', 'title', 'description',\n",
    "    'city', 'location_venue', 'address', 'rating',\n",
    "    'price_min', 'price_currency', 'tags', 'images',\n",
    "    'start_date', 'end_date', 'times', 'extracted_at'\n",
    "]\n",
    "\n",
    "if events_dir.exists() and events_dir.is_dir():\n",
    "    print(f\"\\nüìÅ Events directory: {events_dir}\")\n",
    "    \n",
    "    # Find all individual event JSON files\n",
    "    event_files = sorted(events_dir.glob('event_*.json'))\n",
    "    print(f\"üìñ Found {len(event_files)} individual event JSON files\\n\")\n",
    "    \n",
    "    # Load all event files and filter to important fields only\n",
    "    for idx, event_file in enumerate(event_files, 1):\n",
    "        try:\n",
    "            with open(event_file, 'r', encoding='utf-8') as f:\n",
    "                event_data = json.load(f)\n",
    "                # Keep only important fields\n",
    "                filtered_event = {field: event_data.get(field) for field in IMPORTANT_FIELDS}\n",
    "                all_events.append(filtered_event)\n",
    "            \n",
    "            if idx % 100 == 0:\n",
    "                print(f\"   Loaded {idx}/{len(event_files)} files...\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error loading {event_file.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if all_events:\n",
    "        # Sort by event_id\n",
    "        all_events.sort(key=lambda x: int(x['event_id']))\n",
    "        print(f\"\\n‚úÖ Loaded {len(all_events)} total events\\n\")\n",
    "        print(f\"üìã Fields in combined JSON: {', '.join(IMPORTANT_FIELDS)}\\n\")\n",
    "        \n",
    "        # Save combined JSON file with only important fields (overwrites previous file)\n",
    "        json_file = DATA_DIR / 'events_combined.json'\n",
    "        with open(json_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_events, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"‚úÖ Saved combined JSON: {json_file.name}\")\n",
    "        print(f\"   Location: {DATA_DIR}\")\n",
    "        print(f\"   File size: {json_file.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "        \n",
    "        # Also save as CSV for easy viewing\n",
    "        csv_data = []\n",
    "        for event in all_events:\n",
    "            csv_data.append({\n",
    "                'event_id': event.get('event_id'),\n",
    "                'title': event.get('title'),\n",
    "                'category': event.get('category'),\n",
    "                'city': event.get('city'),\n",
    "                'location_venue': event.get('location_venue'),\n",
    "                'address': event.get('address'),\n",
    "                'rating': event.get('rating'),\n",
    "                'price_min': event.get('price_min'),\n",
    "                'price_currency': event.get('price_currency'),\n",
    "                'image_count': len(event.get('images', [])),\n",
    "                'tag_count': len(event.get('tags', [])),\n",
    "                'time_slots': len(event.get('times', [])),\n",
    "                'url': event.get('url')\n",
    "            })\n",
    "        \n",
    "        csv_df = pd.DataFrame(csv_data)\n",
    "        csv_file = DATA_DIR / 'events_combined.csv'\n",
    "        csv_df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ Saved combined CSV: {csv_file.name}\")\n",
    "        print(f\"   Location: {DATA_DIR}\")\n",
    "        \n",
    "        # Print data quality summary\n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(\"COMBINED DATA SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Total events: {len(all_events)}\")\n",
    "        \n",
    "        print(f\"\\nData Quality by Field:\")\n",
    "        fields = ['title', 'description', 'city', 'location_venue', 'address',\n",
    "                 'price_min', 'images', 'times', 'tags', 'rating', 'start_date']\n",
    "        for field in fields:\n",
    "            count = sum(1 for e in all_events if e.get(field) and \n",
    "                       (isinstance(e[field], (int, float)) or \n",
    "                        (isinstance(e[field], (str, list)) and len(str(e[field])) > 0)))\n",
    "            pct = (count / len(all_events) * 100) if all_events else 0\n",
    "            print(f\"  {field:20s}: {count:4d}/{len(all_events)} ({pct:5.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nüìÇ Output files created:\")\n",
    "        print(f\"   JSON: {json_file.name}\")\n",
    "        print(f\"   CSV:  {csv_file.name}\")\n",
    "        print(f\"\\n‚úÖ COMBINE COMPLETE!\")\n",
    "    else:\n",
    "        print(\"‚ùå No events loaded from files\")\n",
    "else:\n",
    "    print(f\"‚ùå Events directory not found: {events_dir}\")\n",
    "    print(\"   Please run Step 2 (comprehensive extraction) first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "62b5bb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrity check saved: integrity_report_20251024_144408.json\n",
      "Field completeness CSV: integrity_field_completeness_20251024_144408.csv\n",
      "Total events: 518\n",
      "Unique IDs: 518\n",
      "Duplicate event IDs: 0 (sample: [])\n",
      "Fields with non-trivial missingness (non-empty% < 90%):\n",
      "  - city: 63.9% non-empty\n",
      "  - location_venue: 33.98% non-empty\n",
      "  - rating: 64.29% non-empty\n",
      "  - event_type: 64.29% non-empty\n",
      "  - tags: 64.29% non-empty\n",
      "  - times: 64.29% non-empty\n",
      "Date parse issues: 0\n",
      "URL domain issues: 0\n",
      "Price numeric parse issues: 0\n",
      "Images avg: 2.42, tags avg: 5.49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'file': 'c:\\\\Scrapping\\\\joinnus\\\\notebook\\\\data\\\\events_combined.json',\n",
       " 'checked_at': '2025-10-24T14:44:08.074295',\n",
       " 'total_events': 518,\n",
       " 'unique_event_ids': 518,\n",
       " 'duplicate_event_ids': [],\n",
       " 'field_completeness': {'event_id': {'present_count': 518,\n",
       "   'present_pct': 100.0,\n",
       "   'non_empty_count': 518,\n",
       "   'non_empty_pct': 100.0},\n",
       "  'url': {'present_count': 518,\n",
       "   'present_pct': 100.0,\n",
       "   'non_empty_count': 518,\n",
       "   'non_empty_pct': 100.0},\n",
       "  'category': {'present_count': 518,\n",
       "   'present_pct': 100.0,\n",
       "   'non_empty_count': 518,\n",
       "   'non_empty_pct': 100.0},\n",
       "  'title': {'present_count': 518,\n",
       "   'present_pct': 100.0,\n",
       "   'non_empty_count': 518,\n",
       "   'non_empty_pct': 100.0},\n",
       "  'description': {'present_count': 518,\n",
       "   'present_pct': 100.0,\n",
       "   'non_empty_count': 518,\n",
       "   'non_empty_pct': 100.0},\n",
       "  'city': {'present_count': 518,\n",
       "   'present_pct': 100.0,\n",
       "   'non_empty_count': 331,\n",
       "   'non_empty_pct': 63.9},\n",
       "  'location_venue': {'present_count': 518,\n",
       "   'present_pct': 100.0,\n",
       "   'non_empty_count': 176,\n",
       "   'non_empty_pct': 33.98},\n",
       "  'address': {'present_count': 518,\n",
       "   'present_pct': 100.0,\n",
       "   'non_empty_count': 516,\n",
       "   'non_empty_pct': 99.61},\n",
       "  'rating': {'present_count': 518,\n",
       "   'present_pct': 100.0,\n",
       "   'non_empty_count': 333,\n",
       "   'non_empty_pct': 64.29},\n",
       "  'event_type': {'present_count': 518,\n",
       "   'present_pct': 100.0,\n",
       "   'non_empty_count': 333,\n",
       "   'non_empty_pct': 64.29},\n",
       "  'price_min': {'present_count': 518,\n",
       "   'present_pct': 100.0,\n",
       "   'non_empty_count': 516,\n",
       "   'non_empty_pct': 99.61},\n",
       "  'price_currency': {'present_count': 518,\n",
       "   'present_pct': 100.0,\n",
       "   'non_empty_count': 516,\n",
       "   'non_empty_pct': 99.61},\n",
       "  'tags': {'present_count': 518,\n",
       "   'present_pct': 100.0,\n",
       "   'non_empty_count': 333,\n",
       "   'non_empty_pct': 64.29},\n",
       "  'images': {'present_count': 518,\n",
       "   'present_pct': 100.0,\n",
       "   'non_empty_count': 518,\n",
       "   'non_empty_pct': 100.0},\n",
       "  'start_date': {'present_count': 518,\n",
       "   'present_pct': 100.0,\n",
       "   'non_empty_count': 516,\n",
       "   'non_empty_pct': 99.61},\n",
       "  'end_date': {'present_count': 518,\n",
       "   'present_pct': 100.0,\n",
       "   'non_empty_count': 516,\n",
       "   'non_empty_pct': 99.61},\n",
       "  'times': {'present_count': 518,\n",
       "   'present_pct': 100.0,\n",
       "   'non_empty_count': 333,\n",
       "   'non_empty_pct': 64.29},\n",
       "  'extracted_at': {'present_count': 518,\n",
       "   'present_pct': 100.0,\n",
       "   'non_empty_count': 518,\n",
       "   'non_empty_pct': 100.0}},\n",
       " 'field_type_issues': {},\n",
       " 'dates_parsing_issues': 0,\n",
       " 'url_domain_issues': 0,\n",
       " 'price_stats': {'count': 516,\n",
       "  'min': 0.0,\n",
       "  '25%': 20.0,\n",
       "  'median': 32.0,\n",
       "  '75%': 60.0,\n",
       "  'max': 1190.0,\n",
       "  'mean': 54.28727131782945},\n",
       " 'images_tags_times_stats': {'images': {'count': 518,\n",
       "   'mean': 2.422779922779923,\n",
       "   'median': 2.0,\n",
       "   'max': 5,\n",
       "   'pct_zero': 0.0},\n",
       "  'tags': {'count': 518,\n",
       "   'mean': 5.488416988416988,\n",
       "   'median': 1.0,\n",
       "   'max': 20,\n",
       "   'pct_zero': 35.714285714285715},\n",
       "  'times': {'count': 518,\n",
       "   'mean': 0.7953667953667953,\n",
       "   'median': 1.0,\n",
       "   'max': 5,\n",
       "   'pct_zero': 35.714285714285715}},\n",
       " 'rating_distribution': {'NULL': 185,\n",
       "  'G': 156,\n",
       "  '+18': 104,\n",
       "  'GP-18': 65,\n",
       "  '+14': 6,\n",
       "  '+16': 2},\n",
       " 'samples': {'duplicate_event_ids_sample': [],\n",
       "  'url_domain_issues_sample': [],\n",
       "  'date_parse_issues_sample': [],\n",
       "  'price_parse_issues_sample': []}}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "# Data integrity analysis for events_combined.json\n",
    "# Saves a JSON report and a CSV summary of field completeness to DATA_DIR\n",
    "\n",
    "\n",
    "def _parse_iso(dt_str):\n",
    "    if not dt_str or not isinstance(dt_str, str):\n",
    "        return None\n",
    "    s = dt_str.strip()\n",
    "    # Handle trailing Z or timezone-less strings\n",
    "    if s.endswith('Z'):\n",
    "        s = s[:-1] + '+00:00'\n",
    "    try:\n",
    "        return datetime.fromisoformat(s)\n",
    "    except Exception:\n",
    "        # try removing milliseconds if any odd format\n",
    "        try:\n",
    "            return datetime.fromisoformat(s.split('.')[0])\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "# locate combined JSON (use existing variable if available)\n",
    "combined_path = globals().get('json_file', None) or (DATA_DIR / 'events_combined.json')\n",
    "if not combined_path.exists():\n",
    "    raise FileNotFoundError(f\"Combined JSON not found: {combined_path}\")\n",
    "\n",
    "with open(combined_path, 'r', encoding='utf-8') as f:\n",
    "    events = json.load(f)\n",
    "\n",
    "total = len(events)\n",
    "report = {\n",
    "    'file': str(combined_path),\n",
    "    'checked_at': datetime.now().isoformat(),\n",
    "    'total_events': total,\n",
    "    'unique_event_ids': 0,\n",
    "    'duplicate_event_ids': [],\n",
    "    'field_completeness': {},\n",
    "    'field_type_issues': {},\n",
    "    'dates_parsing_issues': 0,\n",
    "    'url_domain_issues': 0,\n",
    "    'price_stats': {},\n",
    "    'images_tags_times_stats': {},\n",
    "    'rating_distribution': {}\n",
    "}\n",
    "\n",
    "# basic id & duplicates\n",
    "ids = [str(e.get('event_id')) for e in events]\n",
    "unique_ids = set(ids)\n",
    "report['unique_event_ids'] = len(unique_ids)\n",
    "dupes = [eid for eid, cnt in Counter(ids).items() if cnt > 1]\n",
    "report['duplicate_event_ids'] = dupes\n",
    "\n",
    "# fields to analyze\n",
    "fields = ['event_id','url','category','title','description','city','location_venue',\n",
    "          'address','rating','event_type','price_min',\n",
    "          'price_currency','tags','images','start_date','end_date','times','extracted_at']\n",
    "\n",
    "# completeness\n",
    "comp_rows = []\n",
    "for field in fields:\n",
    "    present = 0\n",
    "    non_empty = 0\n",
    "    for e in events:\n",
    "        if field in e:\n",
    "            present += 1\n",
    "            v = e.get(field)\n",
    "            if v is None:\n",
    "                pass\n",
    "            elif isinstance(v, str) and v.strip() == '':\n",
    "                pass\n",
    "            elif isinstance(v, (list, dict)) and len(v) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                non_empty += 1\n",
    "    pct_present = (present / total) * 100 if total else 0\n",
    "    pct_non_empty = (non_empty / total) * 100 if total else 0\n",
    "    report['field_completeness'][field] = {\n",
    "        'present_count': present,\n",
    "        'present_pct': round(pct_present,2),\n",
    "        'non_empty_count': non_empty,\n",
    "        'non_empty_pct': round(pct_non_empty,2)\n",
    "    }\n",
    "    comp_rows.append({\n",
    "        'field': field,\n",
    "        'present_count': present,\n",
    "        'present_pct': round(pct_present,2),\n",
    "        'non_empty_count': non_empty,\n",
    "        'non_empty_pct': round(pct_non_empty,2)\n",
    "    })\n",
    "\n",
    "# type checks & domain checks & date parsing & price collection\n",
    "price_vals = []\n",
    "price_parse_issues = 0\n",
    "date_parse_issues = 0\n",
    "url_issues = 0\n",
    "domains_ok = (JOINNUS_CONFIG['base_domain'], JOINNUS_CONFIG['classic_domain'], 'https://prime.joinnus.com')\n",
    "rating_counter = Counter()\n",
    "images_counts = []\n",
    "tags_counts = []\n",
    "times_counts = []\n",
    "\n",
    "for e in events:\n",
    "    # event_id should be numeric-ish\n",
    "    eid = e.get('event_id')\n",
    "    if eid is not None:\n",
    "        if not str(eid).isdigit():\n",
    "            report['field_type_issues'].setdefault('event_id',0)\n",
    "            report['field_type_issues']['event_id'] += 1\n",
    "\n",
    "    # url domain\n",
    "    url = e.get('url','')\n",
    "    if not any(url.startswith(d) for d in domains_ok):\n",
    "        url_issues += 1\n",
    "\n",
    "    # price checks\n",
    "    pm = e.get('price_min')\n",
    "    if pm is not None and pm != '':\n",
    "        try:\n",
    "            price_vals.append(float(pm))\n",
    "        except Exception:\n",
    "            price_parse_issues += 1\n",
    "\n",
    "    # dates\n",
    "    sd = e.get('start_date')\n",
    "    ed = e.get('end_date')\n",
    "    ps = _parse_iso(sd)\n",
    "    pe = _parse_iso(ed)\n",
    "    if sd and not ps:\n",
    "        date_parse_issues += 1\n",
    "    if ed and not pe:\n",
    "        date_parse_issues += 1\n",
    "\n",
    "    # rating distribution\n",
    "    rating_counter.update([str(e.get('rating')) if e.get('rating') is not None else 'NULL'])\n",
    "\n",
    "    # lists counts\n",
    "    imgs = e.get('images') or []\n",
    "    tags = e.get('tags') or []\n",
    "    times = e.get('times') or []\n",
    "    images_counts.append(len(imgs) if isinstance(imgs, (list,tuple)) else 0)\n",
    "    tags_counts.append(len(tags) if isinstance(tags, (list,tuple)) else 0)\n",
    "    times_counts.append(len(times) if isinstance(times, (list,tuple)) else 0)\n",
    "\n",
    "report['dates_parsing_issues'] = date_parse_issues\n",
    "report['url_domain_issues'] = url_issues\n",
    "\n",
    "# price stats\n",
    "if price_vals:\n",
    "    s = pd.Series(price_vals)\n",
    "    report['price_stats'] = {\n",
    "        'count': int(s.count()),\n",
    "        'min': float(s.min()),\n",
    "        '25%': float(s.quantile(0.25)),\n",
    "        'median': float(s.median()),\n",
    "        '75%': float(s.quantile(0.75)),\n",
    "        'max': float(s.max()),\n",
    "        'mean': float(s.mean())\n",
    "    }\n",
    "else:\n",
    "    report['price_stats'] = {'count': 0}\n",
    "\n",
    "# images/tags/times stats\n",
    "def _summmary_list_stats(lst):\n",
    "    s = pd.Series(lst)\n",
    "    return {\n",
    "        'count': int(s.count()),\n",
    "        'mean': float(s.mean()) if len(s) else 0,\n",
    "        'median': float(s.median()) if len(s) else 0,\n",
    "        'max': int(s.max()) if len(s) else 0,\n",
    "        'pct_zero': float((s==0).sum() / len(s) * 100) if len(s) else 0\n",
    "    }\n",
    "\n",
    "report['images_tags_times_stats'] = {\n",
    "    'images': _summmary_list_stats(images_counts),\n",
    "    'tags': _summmary_list_stats(tags_counts),\n",
    "    'times': _summmary_list_stats(times_counts)\n",
    "}\n",
    "\n",
    "# rating distribution\n",
    "report['rating_distribution'] = dict(rating_counter.most_common())\n",
    "\n",
    "# record type issues counts found earlier\n",
    "if price_parse_issues:\n",
    "    report['field_type_issues']['price_min_parse_errors'] = price_parse_issues\n",
    "\n",
    "# sample offending records (small samples)\n",
    "sample_issues = {\n",
    "    'duplicate_event_ids_sample': dupes[:10],\n",
    "    'url_domain_issues_sample': [],\n",
    "    'date_parse_issues_sample': [],\n",
    "    'price_parse_issues_sample': []\n",
    "}\n",
    "\n",
    "for e in events:\n",
    "    if len(sample_issues['url_domain_issues_sample']) < 10:\n",
    "        u = e.get('url','')\n",
    "        if not any(u.startswith(d) for d in domains_ok):\n",
    "            sample_issues['url_domain_issues_sample'].append({'event_id': e.get('event_id'), 'url': u})\n",
    "    if len(sample_issues['date_parse_issues_sample']) < 10:\n",
    "        sd = e.get('start_date')\n",
    "        ed = e.get('end_date')\n",
    "        if (sd and not _parse_iso(sd)) or (ed and not _parse_iso(ed)):\n",
    "            sample_issues['date_parse_issues_sample'].append({'event_id': e.get('event_id'), 'start_date': sd, 'end_date': ed})\n",
    "    if len(sample_issues['price_parse_issues_sample']) < 10:\n",
    "        pm = e.get('price_min')\n",
    "        if pm not in (None, ''):\n",
    "            try:\n",
    "                float(pm)\n",
    "            except Exception:\n",
    "                sample_issues['price_parse_issues_sample'].append({'event_id': e.get('event_id'), 'price_min': pm})\n",
    "\n",
    "report['samples'] = sample_issues\n",
    "\n",
    "# save report\n",
    "ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "out_json = DATA_DIR / f'integrity_report_{ts}.json'\n",
    "out_csv = DATA_DIR / f'integrity_field_completeness_{ts}.csv'\n",
    "\n",
    "with open(out_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "pd.DataFrame(comp_rows).to_csv(out_csv, index=False, encoding='utf-8')\n",
    "\n",
    "# Print concise summary\n",
    "print(f\"Integrity check saved: {out_json.name}\")\n",
    "print(f\"Field completeness CSV: {out_csv.name}\")\n",
    "print(f\"Total events: {total}\")\n",
    "print(f\"Unique IDs: {report['unique_event_ids']}\")\n",
    "print(f\"Duplicate event IDs: {len(report['duplicate_event_ids'])} (sample: {report['duplicate_event_ids'][:5]})\")\n",
    "print(f\"Fields with non-trivial missingness (non-empty% < 90%):\")\n",
    "for frow in comp_rows:\n",
    "    if frow['non_empty_pct'] < 90:\n",
    "        print(f\"  - {frow['field']}: {frow['non_empty_pct']}% non-empty\")\n",
    "\n",
    "print(f\"Date parse issues: {report['dates_parsing_issues']}\")\n",
    "print(f\"URL domain issues: {report['url_domain_issues']}\")\n",
    "print(f\"Price numeric parse issues: {report['field_type_issues'].get('price_min_parse_errors', 0)}\")\n",
    "print(f\"Images avg: {report['images_tags_times_stats']['images']['mean']:.2f}, tags avg: {report['images_tags_times_stats']['tags']['mean']:.2f}\")\n",
    "\n",
    "# expose report variable for interactive inspection\n",
    "report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7a08f8",
   "metadata": {},
   "source": [
    "## üì¶ Step 3: Store Events in MongoDB\n",
    "\n",
    "Upload extracted event data to MongoDB database, checking by event ID to avoid duplicates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8c59d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MongoDBEventStore class defined\n"
     ]
    }
   ],
   "source": [
    "class MongoDBEventStore:\n",
    "    \"\"\"Manages MongoDB operations for event storage\"\"\"\n",
    "    \n",
    "    def __init__(self, mongo_config):\n",
    "        self.config = mongo_config\n",
    "        self.client = None\n",
    "        self.db = None\n",
    "        self.collection = None\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        print(\"üóÑÔ∏è Initializing MongoDB Event Store\")\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to MongoDB\"\"\"\n",
    "        try:\n",
    "            self.client = MongoClient(self.config['uri'], serverSelectionTimeoutMS=5000)\n",
    "            # Test connection\n",
    "            self.client.admin.command('ping')\n",
    "            self.db = self.client[self.config['database']]\n",
    "            self.collection = self.db[self.config['collection']]\n",
    "            \n",
    "            # Create index on event_id for faster lookups\n",
    "            self.collection.create_index('event_id', unique=False)\n",
    "            \n",
    "            print(f\"‚úÖ Connected to MongoDB\")\n",
    "            print(f\"   Database: {self.config['database']}\")\n",
    "            print(f\"   Collection: {self.config['collection']}\")\n",
    "            return True\n",
    "            \n",
    "        except ConnectionFailure as e:\n",
    "            print(f\"‚ùå Failed to connect to MongoDB: {e}\")\n",
    "            print(f\"   Make sure MongoDB is running at {self.config['uri']}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error connecting to MongoDB: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_existing_ids(self):\n",
    "        \"\"\"Get all existing event IDs in collection\"\"\"\n",
    "        try:\n",
    "            if self.collection is None:\n",
    "                return set()\n",
    "            existing_ids = set()\n",
    "            for doc in self.collection.find({}, {'event_id': 1}):\n",
    "                if 'event_id' in doc:\n",
    "                    existing_ids.add(str(doc['event_id']))\n",
    "            print(f\"üìä Found {len(existing_ids)} existing events in MongoDB\")\n",
    "            return existing_ids\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error fetching existing IDs: {e}\")\n",
    "            return set()\n",
    "    \n",
    "    def store_event(self, event_data):\n",
    "        \"\"\"Store a single event in MongoDB\"\"\"\n",
    "        try:\n",
    "            if self.collection is None:\n",
    "                return False\n",
    "            \n",
    "            # Use replace_one with upsert to avoid duplicates\n",
    "            result = self.collection.replace_one(\n",
    "                {'event_id': event_data['event_id']},\n",
    "                event_data,\n",
    "                upsert=True\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error storing event {event_data.get('event_id')}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def store_events_batch(self, events, skip_existing=True):\n",
    "        \"\"\"\n",
    "        Store multiple events in MongoDB with duplicate checking\n",
    "        \n",
    "        Args:\n",
    "            events: List of event dictionaries\n",
    "            skip_existing: If True, skip events already in database\n",
    "            \n",
    "        Returns:\n",
    "            dict: Statistics about insertion\n",
    "        \"\"\"\n",
    "        if self.collection is None:\n",
    "            print(\"‚ùå Not connected to MongoDB\")\n",
    "            return None\n",
    "        \n",
    "        # Get existing IDs if we're skipping\n",
    "        existing_ids = set()\n",
    "        if skip_existing:\n",
    "            existing_ids = self.get_existing_ids()\n",
    "        \n",
    "        stats = {\n",
    "            'total': len(events),\n",
    "            'inserted': 0,\n",
    "            'updated': 0,\n",
    "            'skipped': 0,\n",
    "            'failed': 0\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüì§ Starting batch upload to MongoDB...\")\n",
    "        print(f\"   Total events: {stats['total']}\")\n",
    "        if skip_existing:\n",
    "            print(f\"   Existing in DB: {len(existing_ids)}\")\n",
    "            print(f\"   New events: {stats['total'] - len(existing_ids)}\\n\")\n",
    "        \n",
    "        for idx, event in enumerate(events, 1):\n",
    "            event_id = str(event.get('event_id'))\n",
    "            \n",
    "            # Skip if already exists\n",
    "            if skip_existing and event_id in existing_ids:\n",
    "                stats['skipped'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Show progress\n",
    "            progress_pct = (idx / stats['total'] * 100)\n",
    "            print(f\"[{idx:4d}/{stats['total']}] ({progress_pct:5.1f}%) Event {event_id}...\", end='', flush=True)\n",
    "            \n",
    "            try:\n",
    "                # Store event (upsert = insert if not exists, update if exists)\n",
    "                result = self.collection.replace_one(\n",
    "                    {'event_id': event_id},\n",
    "                    event,\n",
    "                    upsert=True\n",
    "                )\n",
    "                \n",
    "                if result.upserted_id:\n",
    "                    stats['inserted'] += 1\n",
    "                    print(f\" ‚úì Inserted\")\n",
    "                else:\n",
    "                    stats['updated'] += 1\n",
    "                    print(f\" ‚Üª Updated\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                stats['failed'] += 1\n",
    "                print(f\" ‚ö†Ô∏è Error: {str(e)[:50]}\")\n",
    "                self.logger.error(f\"Error storing event {event_id}: {e}\")\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close MongoDB connection\"\"\"\n",
    "        if self.client is not None:\n",
    "            self.client.close()\n",
    "            print(\"‚úÖ MongoDB connection closed\")\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get collection statistics\"\"\"\n",
    "        try:\n",
    "            if self.collection is None:\n",
    "                return None\n",
    "            count = self.collection.count_documents({})\n",
    "            return {\n",
    "                'total_documents': count,\n",
    "                'collection_name': self.config['collection'],\n",
    "                'database_name': self.config['database']\n",
    "            }\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting stats: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "print(\"‚úÖ MongoDBEventStore class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed82c1",
   "metadata": {},
   "source": [
    "## ‚ñ∂Ô∏è Execute Step 3: Upload to MongoDB\n",
    "\n",
    "Run this to load the final events JSON and store in MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6431a1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found events directory: c:\\Scrapping\\joinnus\\notebook\\data\\events\n",
      "üìñ Found 518 individual event JSON files\n",
      "\n",
      "‚úÖ Loaded 518 events from individual files\n",
      "\n",
      "üóÑÔ∏è Initializing MongoDB Event Store\n",
      "‚úÖ Connected to MongoDB\n",
      "   Database: recommendations-system\n",
      "   Collection: events\n",
      "üìä Found 0 existing events in MongoDB\n",
      "\n",
      "üì§ Starting batch upload to MongoDB...\n",
      "   Total events: 518\n",
      "   Existing in DB: 0\n",
      "   New events: 518\n",
      "\n",
      "[   1/518] (  0.2%) Event 2025...‚úÖ Connected to MongoDB\n",
      "   Database: recommendations-system\n",
      "   Collection: events\n",
      "üìä Found 0 existing events in MongoDB\n",
      "\n",
      "üì§ Starting batch upload to MongoDB...\n",
      "   Total events: 518\n",
      "   Existing in DB: 0\n",
      "   New events: 518\n",
      "\n",
      "[   1/518] (  0.2%) Event 2025... ‚úì Inserted\n",
      "[   2/518] (  0.4%) Event 52591... ‚úì Inserted\n",
      "[   2/518] (  0.4%) Event 52591... ‚úì Inserted\n",
      "[   3/518] (  0.6%) Event 58430... ‚úì Inserted\n",
      "[   3/518] (  0.6%) Event 58430... ‚úì Inserted\n",
      "[   4/518] (  0.8%) Event 58431... ‚úì Inserted\n",
      "[   4/518] (  0.8%) Event 58431... ‚úì Inserted\n",
      "[   5/518] (  1.0%) Event 58435... ‚úì Inserted\n",
      "[   5/518] (  1.0%) Event 58435... ‚úì Inserted\n",
      "[   6/518] (  1.2%) Event 58436... ‚úì Inserted\n",
      "[   6/518] (  1.2%) Event 58436... ‚úì Inserted\n",
      "[   7/518] (  1.4%) Event 58482... ‚úì Inserted\n",
      "[   7/518] (  1.4%) Event 58482... ‚úì Inserted\n",
      "[   8/518] (  1.5%) Event 58490... ‚úì Inserted\n",
      "[   8/518] (  1.5%) Event 58490... ‚úì Inserted\n",
      "[   9/518] (  1.7%) Event 58491... ‚úì Inserted\n",
      "[   9/518] (  1.7%) Event 58491... ‚úì Inserted\n",
      "[  10/518] (  1.9%) Event 58492... ‚úì Inserted\n",
      "[  10/518] (  1.9%) Event 58492... ‚úì Inserted\n",
      "[  11/518] (  2.1%) Event 58493... ‚úì Inserted\n",
      "[  11/518] (  2.1%) Event 58493... ‚úì Inserted\n",
      "[  12/518] (  2.3%) Event 58584... ‚úì Inserted\n",
      "[  12/518] (  2.3%) Event 58584... ‚úì Inserted\n",
      "[  13/518] (  2.5%) Event 58585... ‚úì Inserted\n",
      "[  13/518] (  2.5%) Event 58585... ‚úì Inserted\n",
      "[  14/518] (  2.7%) Event 58586... ‚úì Inserted\n",
      "[  14/518] (  2.7%) Event 58586... ‚úì Inserted\n",
      "[  15/518] (  2.9%) Event 58589... ‚úì Inserted\n",
      "[  15/518] (  2.9%) Event 58589... ‚úì Inserted\n",
      "[  16/518] (  3.1%) Event 58590... ‚úì Inserted\n",
      "[  16/518] (  3.1%) Event 58590... ‚úì Inserted\n",
      "[  17/518] (  3.3%) Event 58656... ‚úì Inserted\n",
      "[  17/518] (  3.3%) Event 58656... ‚úì Inserted\n",
      "[  18/518] (  3.5%) Event 58657... ‚úì Inserted\n",
      "[  18/518] (  3.5%) Event 58657... ‚úì Inserted\n",
      "[  19/518] (  3.7%) Event 58667... ‚úì Inserted\n",
      "[  19/518] (  3.7%) Event 58667... ‚úì Inserted\n",
      "[  20/518] (  3.9%) Event 58668... ‚úì Inserted\n",
      "[  20/518] (  3.9%) Event 58668... ‚úì Inserted\n",
      "[  21/518] (  4.1%) Event 59615... ‚úì Inserted\n",
      "[  21/518] (  4.1%) Event 59615... ‚úì Inserted\n",
      "[  22/518] (  4.2%) Event 61879... ‚úì Inserted\n",
      "[  22/518] (  4.2%) Event 61879... ‚úì Inserted\n",
      "[  23/518] (  4.4%) Event 64727... ‚úì Inserted\n",
      "[  23/518] (  4.4%) Event 64727... ‚úì Inserted\n",
      "[  24/518] (  4.6%) Event 65796... ‚úì Inserted\n",
      "[  24/518] (  4.6%) Event 65796... ‚úì Inserted\n",
      "[  25/518] (  4.8%) Event 65980... ‚úì Inserted\n",
      "[  25/518] (  4.8%) Event 65980... ‚úì Inserted\n",
      "[  26/518] (  5.0%) Event 66170... ‚úì Inserted\n",
      "[  26/518] (  5.0%) Event 66170... ‚úì Inserted\n",
      "[  27/518] (  5.2%) Event 66394... ‚úì Inserted\n",
      "[  27/518] (  5.2%) Event 66394... ‚úì Inserted\n",
      "[  28/518] (  5.4%) Event 66447... ‚úì Inserted\n",
      "[  28/518] (  5.4%) Event 66447... ‚úì Inserted\n",
      "[  29/518] (  5.6%) Event 66498... ‚úì Inserted\n",
      "[  29/518] (  5.6%) Event 66498... ‚úì Inserted\n",
      "[  30/518] (  5.8%) Event 66560... ‚úì Inserted\n",
      "[  30/518] (  5.8%) Event 66560... ‚úì Inserted\n",
      "[  31/518] (  6.0%) Event 66683... ‚úì Inserted\n",
      "[  31/518] (  6.0%) Event 66683... ‚úì Inserted\n",
      "[  32/518] (  6.2%) Event 66733... ‚úì Inserted\n",
      "[  32/518] (  6.2%) Event 66733... ‚úì Inserted\n",
      "[  33/518] (  6.4%) Event 66886... ‚úì Inserted\n",
      "[  33/518] (  6.4%) Event 66886... ‚úì Inserted\n",
      "[  34/518] (  6.6%) Event 66917... ‚úì Inserted\n",
      "[  34/518] (  6.6%) Event 66917... ‚úì Inserted\n",
      "[  35/518] (  6.8%) Event 66943... ‚úì Inserted\n",
      "[  35/518] (  6.8%) Event 66943... ‚úì Inserted\n",
      "[  36/518] (  6.9%) Event 66944... ‚úì Inserted\n",
      "[  36/518] (  6.9%) Event 66944... ‚úì Inserted\n",
      "[  37/518] (  7.1%) Event 66945... ‚úì Inserted\n",
      "[  37/518] (  7.1%) Event 66945... ‚úì Inserted\n",
      "[  38/518] (  7.3%) Event 67029... ‚úì Inserted\n",
      "[  38/518] (  7.3%) Event 67029... ‚úì Inserted\n",
      "[  39/518] (  7.5%) Event 67032... ‚úì Inserted\n",
      "[  39/518] (  7.5%) Event 67032... ‚úì Inserted\n",
      "[  40/518] (  7.7%) Event 67061... ‚úì Inserted\n",
      "[  40/518] (  7.7%) Event 67061... ‚úì Inserted\n",
      "[  41/518] (  7.9%) Event 67250... ‚úì Inserted\n",
      "[  41/518] (  7.9%) Event 67250... ‚úì Inserted\n",
      "[  42/518] (  8.1%) Event 67339... ‚úì Inserted\n",
      "[  42/518] (  8.1%) Event 67339... ‚úì Inserted\n",
      "[  43/518] (  8.3%) Event 67826... ‚úì Inserted\n",
      "[  43/518] (  8.3%) Event 67826... ‚úì Inserted\n",
      "[  44/518] (  8.5%) Event 67859... ‚úì Inserted\n",
      "[  44/518] (  8.5%) Event 67859... ‚úì Inserted\n",
      "[  45/518] (  8.7%) Event 68140... ‚úì Inserted\n",
      "[  45/518] (  8.7%) Event 68140... ‚úì Inserted\n",
      "[  46/518] (  8.9%) Event 68690... ‚úì Inserted\n",
      "[  46/518] (  8.9%) Event 68690... ‚úì Inserted\n",
      "[  47/518] (  9.1%) Event 69083... ‚úì Inserted\n",
      "[  47/518] (  9.1%) Event 69083... ‚úì Inserted\n",
      "[  48/518] (  9.3%) Event 69088... ‚úì Inserted\n",
      "[  48/518] (  9.3%) Event 69088... ‚úì Inserted\n",
      "[  49/518] (  9.5%) Event 69106... ‚úì Inserted\n",
      "[  49/518] (  9.5%) Event 69106... ‚úì Inserted\n",
      "[  50/518] (  9.7%) Event 69316... ‚úì Inserted\n",
      "[  50/518] (  9.7%) Event 69316... ‚úì Inserted\n",
      "[  51/518] (  9.8%) Event 69585... ‚úì Inserted\n",
      "[  51/518] (  9.8%) Event 69585... ‚úì Inserted\n",
      "[  52/518] ( 10.0%) Event 69598... ‚úì Inserted\n",
      "[  52/518] ( 10.0%) Event 69598... ‚úì Inserted\n",
      "[  53/518] ( 10.2%) Event 69653... ‚úì Inserted\n",
      "[  53/518] ( 10.2%) Event 69653... ‚úì Inserted\n",
      "[  54/518] ( 10.4%) Event 69689... ‚úì Inserted\n",
      "[  54/518] ( 10.4%) Event 69689... ‚úì Inserted\n",
      "[  55/518] ( 10.6%) Event 69801... ‚úì Inserted\n",
      "[  55/518] ( 10.6%) Event 69801... ‚úì Inserted\n",
      "[  56/518] ( 10.8%) Event 69849... ‚úì Inserted\n",
      "[  56/518] ( 10.8%) Event 69849... ‚úì Inserted\n",
      "[  57/518] ( 11.0%) Event 69979... ‚úì Inserted\n",
      "[  57/518] ( 11.0%) Event 69979... ‚úì Inserted\n",
      "[  58/518] ( 11.2%) Event 70030... ‚úì Inserted\n",
      "[  58/518] ( 11.2%) Event 70030... ‚úì Inserted\n",
      "[  59/518] ( 11.4%) Event 70089... ‚úì Inserted\n",
      "[  59/518] ( 11.4%) Event 70089... ‚úì Inserted\n",
      "[  60/518] ( 11.6%) Event 70106... ‚úì Inserted\n",
      "[  60/518] ( 11.6%) Event 70106... ‚úì Inserted\n",
      "[  61/518] ( 11.8%) Event 70117... ‚úì Inserted\n",
      "[  61/518] ( 11.8%) Event 70117... ‚úì Inserted\n",
      "[  62/518] ( 12.0%) Event 70146... ‚úì Inserted\n",
      "[  62/518] ( 12.0%) Event 70146... ‚úì Inserted\n",
      "[  63/518] ( 12.2%) Event 70241... ‚úì Inserted\n",
      "[  63/518] ( 12.2%) Event 70241... ‚úì Inserted\n",
      "[  64/518] ( 12.4%) Event 70242... ‚úì Inserted\n",
      "[  64/518] ( 12.4%) Event 70242... ‚úì Inserted\n",
      "[  65/518] ( 12.5%) Event 70357... ‚úì Inserted\n",
      "[  65/518] ( 12.5%) Event 70357... ‚úì Inserted\n",
      "[  66/518] ( 12.7%) Event 70407... ‚úì Inserted\n",
      "[  66/518] ( 12.7%) Event 70407... ‚úì Inserted\n",
      "[  67/518] ( 12.9%) Event 70439... ‚úì Inserted\n",
      "[  67/518] ( 12.9%) Event 70439... ‚úì Inserted\n",
      "[  68/518] ( 13.1%) Event 70462... ‚úì Inserted\n",
      "[  68/518] ( 13.1%) Event 70462... ‚úì Inserted\n",
      "[  69/518] ( 13.3%) Event 70488... ‚úì Inserted\n",
      "[  69/518] ( 13.3%) Event 70488... ‚úì Inserted\n",
      "[  70/518] ( 13.5%) Event 70492... ‚úì Inserted\n",
      "[  70/518] ( 13.5%) Event 70492... ‚úì Inserted\n",
      "[  71/518] ( 13.7%) Event 70504... ‚úì Inserted\n",
      "[  71/518] ( 13.7%) Event 70504... ‚úì Inserted\n",
      "[  72/518] ( 13.9%) Event 70540... ‚úì Inserted\n",
      "[  72/518] ( 13.9%) Event 70540... ‚úì Inserted\n",
      "[  73/518] ( 14.1%) Event 70542... ‚úì Inserted\n",
      "[  73/518] ( 14.1%) Event 70542... ‚úì Inserted\n",
      "[  74/518] ( 14.3%) Event 70603... ‚úì Inserted\n",
      "[  74/518] ( 14.3%) Event 70603... ‚úì Inserted\n",
      "[  75/518] ( 14.5%) Event 70652... ‚úì Inserted\n",
      "[  75/518] ( 14.5%) Event 70652... ‚úì Inserted\n",
      "[  76/518] ( 14.7%) Event 70713... ‚úì Inserted\n",
      "[  76/518] ( 14.7%) Event 70713... ‚úì Inserted\n",
      "[  77/518] ( 14.9%) Event 70715... ‚úì Inserted\n",
      "[  77/518] ( 14.9%) Event 70715... ‚úì Inserted\n",
      "[  78/518] ( 15.1%) Event 70744... ‚úì Inserted\n",
      "[  78/518] ( 15.1%) Event 70744... ‚úì Inserted\n",
      "[  79/518] ( 15.3%) Event 70810... ‚úì Inserted\n",
      "[  79/518] ( 15.3%) Event 70810... ‚úì Inserted\n",
      "[  80/518] ( 15.4%) Event 70826... ‚úì Inserted\n",
      "[  80/518] ( 15.4%) Event 70826... ‚úì Inserted\n",
      "[  81/518] ( 15.6%) Event 70847... ‚úì Inserted\n",
      "[  81/518] ( 15.6%) Event 70847... ‚úì Inserted\n",
      "[  82/518] ( 15.8%) Event 70926... ‚úì Inserted\n",
      "[  82/518] ( 15.8%) Event 70926... ‚úì Inserted\n",
      "[  83/518] ( 16.0%) Event 70933... ‚úì Inserted\n",
      "[  83/518] ( 16.0%) Event 70933... ‚úì Inserted\n",
      "[  84/518] ( 16.2%) Event 70936... ‚úì Inserted\n",
      "[  84/518] ( 16.2%) Event 70936... ‚úì Inserted\n",
      "[  85/518] ( 16.4%) Event 70945... ‚úì Inserted\n",
      "[  85/518] ( 16.4%) Event 70945... ‚úì Inserted\n",
      "[  86/518] ( 16.6%) Event 70950... ‚úì Inserted\n",
      "[  86/518] ( 16.6%) Event 70950... ‚úì Inserted\n",
      "[  87/518] ( 16.8%) Event 70958... ‚úì Inserted\n",
      "[  87/518] ( 16.8%) Event 70958... ‚úì Inserted\n",
      "[  88/518] ( 17.0%) Event 70962... ‚úì Inserted\n",
      "[  88/518] ( 17.0%) Event 70962... ‚úì Inserted\n",
      "[  89/518] ( 17.2%) Event 70992... ‚úì Inserted\n",
      "[  89/518] ( 17.2%) Event 70992... ‚úì Inserted\n",
      "[  90/518] ( 17.4%) Event 71010... ‚úì Inserted\n",
      "[  90/518] ( 17.4%) Event 71010... ‚úì Inserted\n",
      "[  91/518] ( 17.6%) Event 71012... ‚úì Inserted\n",
      "[  91/518] ( 17.6%) Event 71012... ‚úì Inserted\n",
      "[  92/518] ( 17.8%) Event 71045... ‚úì Inserted\n",
      "[  92/518] ( 17.8%) Event 71045... ‚úì Inserted\n",
      "[  93/518] ( 18.0%) Event 71061... ‚úì Inserted\n",
      "[  93/518] ( 18.0%) Event 71061... ‚úì Inserted\n",
      "[  94/518] ( 18.1%) Event 71080... ‚úì Inserted\n",
      "[  94/518] ( 18.1%) Event 71080... ‚úì Inserted\n",
      "[  95/518] ( 18.3%) Event 71081... ‚úì Inserted\n",
      "[  95/518] ( 18.3%) Event 71081... ‚úì Inserted\n",
      "[  96/518] ( 18.5%) Event 71084... ‚úì Inserted\n",
      "[  96/518] ( 18.5%) Event 71084... ‚úì Inserted\n",
      "[  97/518] ( 18.7%) Event 71098... ‚úì Inserted\n",
      "[  97/518] ( 18.7%) Event 71098... ‚úì Inserted\n",
      "[  98/518] ( 18.9%) Event 71104... ‚úì Inserted\n",
      "[  98/518] ( 18.9%) Event 71104... ‚úì Inserted\n",
      "[  99/518] ( 19.1%) Event 71110... ‚úì Inserted\n",
      "[  99/518] ( 19.1%) Event 71110... ‚úì Inserted\n",
      "[ 100/518] ( 19.3%) Event 71115... ‚úì Inserted\n",
      "[ 100/518] ( 19.3%) Event 71115... ‚úì Inserted\n",
      "[ 101/518] ( 19.5%) Event 71117... ‚úì Inserted\n",
      "[ 101/518] ( 19.5%) Event 71117... ‚úì Inserted\n",
      "[ 102/518] ( 19.7%) Event 71118... ‚úì Inserted\n",
      "[ 102/518] ( 19.7%) Event 71118... ‚úì Inserted\n",
      "[ 103/518] ( 19.9%) Event 71122... ‚úì Inserted\n",
      "[ 103/518] ( 19.9%) Event 71122... ‚úì Inserted\n",
      "[ 104/518] ( 20.1%) Event 71133... ‚úì Inserted\n",
      "[ 104/518] ( 20.1%) Event 71133... ‚úì Inserted\n",
      "[ 105/518] ( 20.3%) Event 71172... ‚úì Inserted\n",
      "[ 105/518] ( 20.3%) Event 71172... ‚úì Inserted\n",
      "[ 106/518] ( 20.5%) Event 71182... ‚úì Inserted\n",
      "[ 106/518] ( 20.5%) Event 71182... ‚úì Inserted\n",
      "[ 107/518] ( 20.7%) Event 71187... ‚úì Inserted\n",
      "[ 107/518] ( 20.7%) Event 71187... ‚úì Inserted\n",
      "[ 108/518] ( 20.8%) Event 71206... ‚úì Inserted\n",
      "[ 108/518] ( 20.8%) Event 71206... ‚úì Inserted\n",
      "[ 109/518] ( 21.0%) Event 71210... ‚úì Inserted\n",
      "[ 109/518] ( 21.0%) Event 71210... ‚úì Inserted\n",
      "[ 110/518] ( 21.2%) Event 71221... ‚úì Inserted\n",
      "[ 110/518] ( 21.2%) Event 71221... ‚úì Inserted\n",
      "[ 111/518] ( 21.4%) Event 71226... ‚úì Inserted\n",
      "[ 111/518] ( 21.4%) Event 71226... ‚úì Inserted\n",
      "[ 112/518] ( 21.6%) Event 71235... ‚úì Inserted\n",
      "[ 112/518] ( 21.6%) Event 71235... ‚úì Inserted\n",
      "[ 113/518] ( 21.8%) Event 71243... ‚úì Inserted\n",
      "[ 113/518] ( 21.8%) Event 71243... ‚úì Inserted\n",
      "[ 114/518] ( 22.0%) Event 71249... ‚úì Inserted\n",
      "[ 114/518] ( 22.0%) Event 71249... ‚úì Inserted\n",
      "[ 115/518] ( 22.2%) Event 71256... ‚úì Inserted\n",
      "[ 115/518] ( 22.2%) Event 71256... ‚úì Inserted\n",
      "[ 116/518] ( 22.4%) Event 71301... ‚úì Inserted\n",
      "[ 116/518] ( 22.4%) Event 71301... ‚úì Inserted\n",
      "[ 117/518] ( 22.6%) Event 71308... ‚úì Inserted\n",
      "[ 117/518] ( 22.6%) Event 71308... ‚úì Inserted\n",
      "[ 118/518] ( 22.8%) Event 71321... ‚úì Inserted\n",
      "[ 118/518] ( 22.8%) Event 71321... ‚úì Inserted\n",
      "[ 119/518] ( 23.0%) Event 71330... ‚úì Inserted\n",
      "[ 119/518] ( 23.0%) Event 71330... ‚úì Inserted\n",
      "[ 120/518] ( 23.2%) Event 71347... ‚úì Inserted\n",
      "[ 120/518] ( 23.2%) Event 71347... ‚úì Inserted\n",
      "[ 121/518] ( 23.4%) Event 71370... ‚úì Inserted\n",
      "[ 121/518] ( 23.4%) Event 71370... ‚úì Inserted\n",
      "[ 122/518] ( 23.6%) Event 71374... ‚úì Inserted\n",
      "[ 122/518] ( 23.6%) Event 71374... ‚úì Inserted\n",
      "[ 123/518] ( 23.7%) Event 71375... ‚úì Inserted\n",
      "[ 123/518] ( 23.7%) Event 71375... ‚úì Inserted\n",
      "[ 124/518] ( 23.9%) Event 71378... ‚úì Inserted\n",
      "[ 124/518] ( 23.9%) Event 71378... ‚úì Inserted\n",
      "[ 125/518] ( 24.1%) Event 71381... ‚úì Inserted\n",
      "[ 125/518] ( 24.1%) Event 71381... ‚úì Inserted\n",
      "[ 126/518] ( 24.3%) Event 71421... ‚úì Inserted\n",
      "[ 126/518] ( 24.3%) Event 71421... ‚úì Inserted\n",
      "[ 127/518] ( 24.5%) Event 71423... ‚úì Inserted\n",
      "[ 127/518] ( 24.5%) Event 71423... ‚úì Inserted\n",
      "[ 128/518] ( 24.7%) Event 71425... ‚úì Inserted\n",
      "[ 128/518] ( 24.7%) Event 71425... ‚úì Inserted\n",
      "[ 129/518] ( 24.9%) Event 71426... ‚úì Inserted\n",
      "[ 129/518] ( 24.9%) Event 71426... ‚úì Inserted\n",
      "[ 130/518] ( 25.1%) Event 71449... ‚úì Inserted\n",
      "[ 130/518] ( 25.1%) Event 71449... ‚úì Inserted\n",
      "[ 131/518] ( 25.3%) Event 71467... ‚úì Inserted\n",
      "[ 131/518] ( 25.3%) Event 71467... ‚úì Inserted\n",
      "[ 132/518] ( 25.5%) Event 71471... ‚úì Inserted\n",
      "[ 132/518] ( 25.5%) Event 71471... ‚úì Inserted\n",
      "[ 133/518] ( 25.7%) Event 71475... ‚úì Inserted\n",
      "[ 133/518] ( 25.7%) Event 71475... ‚úì Inserted\n",
      "[ 134/518] ( 25.9%) Event 71477... ‚úì Inserted\n",
      "[ 134/518] ( 25.9%) Event 71477... ‚úì Inserted\n",
      "[ 135/518] ( 26.1%) Event 71479... ‚úì Inserted\n",
      "[ 135/518] ( 26.1%) Event 71479... ‚úì Inserted\n",
      "[ 136/518] ( 26.3%) Event 71484... ‚úì Inserted\n",
      "[ 136/518] ( 26.3%) Event 71484... ‚úì Inserted\n",
      "[ 137/518] ( 26.4%) Event 71486... ‚úì Inserted\n",
      "[ 137/518] ( 26.4%) Event 71486... ‚úì Inserted\n",
      "[ 138/518] ( 26.6%) Event 71490... ‚úì Inserted\n",
      "[ 138/518] ( 26.6%) Event 71490... ‚úì Inserted\n",
      "[ 139/518] ( 26.8%) Event 71494... ‚úì Inserted\n",
      "[ 139/518] ( 26.8%) Event 71494... ‚úì Inserted\n",
      "[ 140/518] ( 27.0%) Event 71497... ‚úì Inserted\n",
      "[ 140/518] ( 27.0%) Event 71497... ‚úì Inserted\n",
      "[ 141/518] ( 27.2%) Event 71509... ‚úì Inserted\n",
      "[ 141/518] ( 27.2%) Event 71509... ‚úì Inserted\n",
      "[ 142/518] ( 27.4%) Event 71514... ‚úì Inserted\n",
      "[ 142/518] ( 27.4%) Event 71514... ‚úì Inserted\n",
      "[ 143/518] ( 27.6%) Event 71516... ‚úì Inserted\n",
      "[ 143/518] ( 27.6%) Event 71516... ‚úì Inserted\n",
      "[ 144/518] ( 27.8%) Event 71532... ‚úì Inserted\n",
      "[ 144/518] ( 27.8%) Event 71532... ‚úì Inserted\n",
      "[ 145/518] ( 28.0%) Event 71539... ‚úì Inserted\n",
      "[ 145/518] ( 28.0%) Event 71539... ‚úì Inserted\n",
      "[ 146/518] ( 28.2%) Event 71551... ‚úì Inserted\n",
      "[ 146/518] ( 28.2%) Event 71551... ‚úì Inserted\n",
      "[ 147/518] ( 28.4%) Event 71556... ‚úì Inserted\n",
      "[ 147/518] ( 28.4%) Event 71556... ‚úì Inserted\n",
      "[ 148/518] ( 28.6%) Event 71558... ‚úì Inserted\n",
      "[ 148/518] ( 28.6%) Event 71558... ‚úì Inserted\n",
      "[ 149/518] ( 28.8%) Event 71559... ‚úì Inserted\n",
      "[ 149/518] ( 28.8%) Event 71559... ‚úì Inserted\n",
      "[ 150/518] ( 29.0%) Event 71571... ‚úì Inserted\n",
      "[ 150/518] ( 29.0%) Event 71571... ‚úì Inserted\n",
      "[ 151/518] ( 29.2%) Event 71603... ‚úì Inserted\n",
      "[ 151/518] ( 29.2%) Event 71603... ‚úì Inserted\n",
      "[ 152/518] ( 29.3%) Event 71605... ‚úì Inserted\n",
      "[ 152/518] ( 29.3%) Event 71605... ‚úì Inserted\n",
      "[ 153/518] ( 29.5%) Event 71614... ‚úì Inserted\n",
      "[ 153/518] ( 29.5%) Event 71614... ‚úì Inserted\n",
      "[ 154/518] ( 29.7%) Event 71626... ‚úì Inserted\n",
      "[ 154/518] ( 29.7%) Event 71626... ‚úì Inserted\n",
      "[ 155/518] ( 29.9%) Event 71630... ‚úì Inserted\n",
      "[ 155/518] ( 29.9%) Event 71630... ‚úì Inserted\n",
      "[ 156/518] ( 30.1%) Event 71640... ‚úì Inserted\n",
      "[ 156/518] ( 30.1%) Event 71640... ‚úì Inserted\n",
      "[ 157/518] ( 30.3%) Event 71644... ‚úì Inserted\n",
      "[ 157/518] ( 30.3%) Event 71644... ‚úì Inserted\n",
      "[ 158/518] ( 30.5%) Event 71646... ‚úì Inserted\n",
      "[ 158/518] ( 30.5%) Event 71646... ‚úì Inserted\n",
      "[ 159/518] ( 30.7%) Event 71649... ‚úì Inserted\n",
      "[ 159/518] ( 30.7%) Event 71649... ‚úì Inserted\n",
      "[ 160/518] ( 30.9%) Event 71654... ‚úì Inserted\n",
      "[ 160/518] ( 30.9%) Event 71654... ‚úì Inserted\n",
      "[ 161/518] ( 31.1%) Event 71660... ‚úì Inserted\n",
      "[ 161/518] ( 31.1%) Event 71660... ‚úì Inserted\n",
      "[ 162/518] ( 31.3%) Event 71674... ‚úì Inserted\n",
      "[ 162/518] ( 31.3%) Event 71674... ‚úì Inserted\n",
      "[ 163/518] ( 31.5%) Event 71686... ‚úì Inserted\n",
      "[ 163/518] ( 31.5%) Event 71686... ‚úì Inserted\n",
      "[ 164/518] ( 31.7%) Event 71701... ‚úì Inserted\n",
      "[ 164/518] ( 31.7%) Event 71701... ‚úì Inserted\n",
      "[ 165/518] ( 31.9%) Event 71702... ‚úì Inserted\n",
      "[ 165/518] ( 31.9%) Event 71702... ‚úì Inserted\n",
      "[ 166/518] ( 32.0%) Event 71703... ‚úì Inserted\n",
      "[ 166/518] ( 32.0%) Event 71703... ‚úì Inserted\n",
      "[ 167/518] ( 32.2%) Event 71705... ‚úì Inserted\n",
      "[ 167/518] ( 32.2%) Event 71705... ‚úì Inserted\n",
      "[ 168/518] ( 32.4%) Event 71713... ‚úì Inserted\n",
      "[ 168/518] ( 32.4%) Event 71713... ‚úì Inserted\n",
      "[ 169/518] ( 32.6%) Event 71735... ‚úì Inserted\n",
      "[ 169/518] ( 32.6%) Event 71735... ‚úì Inserted\n",
      "[ 170/518] ( 32.8%) Event 71736... ‚úì Inserted\n",
      "[ 170/518] ( 32.8%) Event 71736... ‚úì Inserted\n",
      "[ 171/518] ( 33.0%) Event 71737... ‚úì Inserted\n",
      "[ 171/518] ( 33.0%) Event 71737... ‚úì Inserted\n",
      "[ 172/518] ( 33.2%) Event 71738... ‚úì Inserted\n",
      "[ 172/518] ( 33.2%) Event 71738... ‚úì Inserted\n",
      "[ 173/518] ( 33.4%) Event 71741... ‚úì Inserted\n",
      "[ 173/518] ( 33.4%) Event 71741... ‚úì Inserted\n",
      "[ 174/518] ( 33.6%) Event 71762... ‚úì Inserted\n",
      "[ 174/518] ( 33.6%) Event 71762... ‚úì Inserted\n",
      "[ 175/518] ( 33.8%) Event 71766... ‚úì Inserted\n",
      "[ 175/518] ( 33.8%) Event 71766... ‚úì Inserted\n",
      "[ 176/518] ( 34.0%) Event 71777... ‚úì Inserted\n",
      "[ 176/518] ( 34.0%) Event 71777... ‚úì Inserted\n",
      "[ 177/518] ( 34.2%) Event 71778... ‚úì Inserted\n",
      "[ 177/518] ( 34.2%) Event 71778... ‚úì Inserted\n",
      "[ 178/518] ( 34.4%) Event 71779... ‚úì Inserted\n",
      "[ 178/518] ( 34.4%) Event 71779... ‚úì Inserted\n",
      "[ 179/518] ( 34.6%) Event 71783... ‚úì Inserted\n",
      "[ 179/518] ( 34.6%) Event 71783... ‚úì Inserted\n",
      "[ 180/518] ( 34.7%) Event 71784... ‚úì Inserted\n",
      "[ 180/518] ( 34.7%) Event 71784... ‚úì Inserted\n",
      "[ 181/518] ( 34.9%) Event 71786... ‚úì Inserted\n",
      "[ 181/518] ( 34.9%) Event 71786... ‚úì Inserted\n",
      "[ 182/518] ( 35.1%) Event 71787... ‚úì Inserted\n",
      "[ 182/518] ( 35.1%) Event 71787... ‚úì Inserted\n",
      "[ 183/518] ( 35.3%) Event 71791... ‚úì Inserted\n",
      "[ 183/518] ( 35.3%) Event 71791... ‚úì Inserted\n",
      "[ 184/518] ( 35.5%) Event 71794... ‚úì Inserted\n",
      "[ 184/518] ( 35.5%) Event 71794... ‚úì Inserted\n",
      "[ 185/518] ( 35.7%) Event 71795... ‚úì Inserted\n",
      "[ 185/518] ( 35.7%) Event 71795... ‚úì Inserted\n",
      "[ 186/518] ( 35.9%) Event 71798... ‚úì Inserted\n",
      "[ 186/518] ( 35.9%) Event 71798... ‚úì Inserted\n",
      "[ 187/518] ( 36.1%) Event 71800... ‚úì Inserted\n",
      "[ 187/518] ( 36.1%) Event 71800... ‚úì Inserted\n",
      "[ 188/518] ( 36.3%) Event 71802... ‚úì Inserted\n",
      "[ 188/518] ( 36.3%) Event 71802... ‚úì Inserted\n",
      "[ 189/518] ( 36.5%) Event 71805... ‚úì Inserted\n",
      "[ 189/518] ( 36.5%) Event 71805... ‚úì Inserted\n",
      "[ 190/518] ( 36.7%) Event 71808... ‚úì Inserted\n",
      "[ 190/518] ( 36.7%) Event 71808... ‚úì Inserted\n",
      "[ 191/518] ( 36.9%) Event 71815... ‚úì Inserted\n",
      "[ 191/518] ( 36.9%) Event 71815... ‚úì Inserted\n",
      "[ 192/518] ( 37.1%) Event 71824... ‚úì Inserted\n",
      "[ 192/518] ( 37.1%) Event 71824... ‚úì Inserted\n",
      "[ 193/518] ( 37.3%) Event 71826... ‚úì Inserted\n",
      "[ 193/518] ( 37.3%) Event 71826... ‚úì Inserted\n",
      "[ 194/518] ( 37.5%) Event 71831... ‚úì Inserted\n",
      "[ 194/518] ( 37.5%) Event 71831... ‚úì Inserted\n",
      "[ 195/518] ( 37.6%) Event 71839... ‚úì Inserted\n",
      "[ 195/518] ( 37.6%) Event 71839... ‚úì Inserted\n",
      "[ 196/518] ( 37.8%) Event 71840... ‚úì Inserted\n",
      "[ 196/518] ( 37.8%) Event 71840... ‚úì Inserted\n",
      "[ 197/518] ( 38.0%) Event 71850... ‚úì Inserted\n",
      "[ 197/518] ( 38.0%) Event 71850... ‚úì Inserted\n",
      "[ 198/518] ( 38.2%) Event 71852... ‚úì Inserted\n",
      "[ 198/518] ( 38.2%) Event 71852... ‚úì Inserted\n",
      "[ 199/518] ( 38.4%) Event 71854... ‚úì Inserted\n",
      "[ 199/518] ( 38.4%) Event 71854... ‚úì Inserted\n",
      "[ 200/518] ( 38.6%) Event 71855... ‚úì Inserted\n",
      "[ 200/518] ( 38.6%) Event 71855... ‚úì Inserted\n",
      "[ 201/518] ( 38.8%) Event 71867... ‚úì Inserted\n",
      "[ 201/518] ( 38.8%) Event 71867... ‚úì Inserted\n",
      "[ 202/518] ( 39.0%) Event 71868... ‚úì Inserted\n",
      "[ 202/518] ( 39.0%) Event 71868... ‚úì Inserted\n",
      "[ 203/518] ( 39.2%) Event 71870... ‚úì Inserted\n",
      "[ 203/518] ( 39.2%) Event 71870... ‚úì Inserted\n",
      "[ 204/518] ( 39.4%) Event 71871... ‚úì Inserted\n",
      "[ 204/518] ( 39.4%) Event 71871... ‚úì Inserted\n",
      "[ 205/518] ( 39.6%) Event 71874... ‚úì Inserted\n",
      "[ 205/518] ( 39.6%) Event 71874... ‚úì Inserted\n",
      "[ 206/518] ( 39.8%) Event 71880... ‚úì Inserted\n",
      "[ 206/518] ( 39.8%) Event 71880... ‚úì Inserted\n",
      "[ 207/518] ( 40.0%) Event 71883... ‚úì Inserted\n",
      "[ 207/518] ( 40.0%) Event 71883... ‚úì Inserted\n",
      "[ 208/518] ( 40.2%) Event 71887... ‚úì Inserted\n",
      "[ 208/518] ( 40.2%) Event 71887... ‚úì Inserted\n",
      "[ 209/518] ( 40.3%) Event 71890... ‚úì Inserted\n",
      "[ 209/518] ( 40.3%) Event 71890... ‚úì Inserted\n",
      "[ 210/518] ( 40.5%) Event 71891... ‚úì Inserted\n",
      "[ 210/518] ( 40.5%) Event 71891... ‚úì Inserted\n",
      "[ 211/518] ( 40.7%) Event 71893... ‚úì Inserted\n",
      "[ 211/518] ( 40.7%) Event 71893... ‚úì Inserted\n",
      "[ 212/518] ( 40.9%) Event 71894... ‚úì Inserted\n",
      "[ 212/518] ( 40.9%) Event 71894... ‚úì Inserted\n",
      "[ 213/518] ( 41.1%) Event 71895... ‚úì Inserted\n",
      "[ 213/518] ( 41.1%) Event 71895... ‚úì Inserted\n",
      "[ 214/518] ( 41.3%) Event 71900... ‚úì Inserted\n",
      "[ 214/518] ( 41.3%) Event 71900... ‚úì Inserted\n",
      "[ 215/518] ( 41.5%) Event 71904... ‚úì Inserted\n",
      "[ 215/518] ( 41.5%) Event 71904... ‚úì Inserted\n",
      "[ 216/518] ( 41.7%) Event 71907... ‚úì Inserted\n",
      "[ 216/518] ( 41.7%) Event 71907... ‚úì Inserted\n",
      "[ 217/518] ( 41.9%) Event 71918... ‚úì Inserted\n",
      "[ 217/518] ( 41.9%) Event 71918... ‚úì Inserted\n",
      "[ 218/518] ( 42.1%) Event 71919... ‚úì Inserted\n",
      "[ 218/518] ( 42.1%) Event 71919... ‚úì Inserted\n",
      "[ 219/518] ( 42.3%) Event 71921... ‚úì Inserted\n",
      "[ 219/518] ( 42.3%) Event 71921... ‚úì Inserted\n",
      "[ 220/518] ( 42.5%) Event 71926... ‚úì Inserted\n",
      "[ 220/518] ( 42.5%) Event 71926... ‚úì Inserted\n",
      "[ 221/518] ( 42.7%) Event 71929... ‚úì Inserted\n",
      "[ 221/518] ( 42.7%) Event 71929... ‚úì Inserted\n",
      "[ 222/518] ( 42.9%) Event 71930... ‚úì Inserted\n",
      "[ 222/518] ( 42.9%) Event 71930... ‚úì Inserted\n",
      "[ 223/518] ( 43.1%) Event 71931... ‚úì Inserted\n",
      "[ 223/518] ( 43.1%) Event 71931... ‚úì Inserted\n",
      "[ 224/518] ( 43.2%) Event 71932... ‚úì Inserted\n",
      "[ 224/518] ( 43.2%) Event 71932... ‚úì Inserted\n",
      "[ 225/518] ( 43.4%) Event 71935... ‚úì Inserted\n",
      "[ 225/518] ( 43.4%) Event 71935... ‚úì Inserted\n",
      "[ 226/518] ( 43.6%) Event 71936... ‚úì Inserted\n",
      "[ 226/518] ( 43.6%) Event 71936... ‚úì Inserted\n",
      "[ 227/518] ( 43.8%) Event 71937... ‚úì Inserted\n",
      "[ 227/518] ( 43.8%) Event 71937... ‚úì Inserted\n",
      "[ 228/518] ( 44.0%) Event 71939... ‚úì Inserted\n",
      "[ 228/518] ( 44.0%) Event 71939... ‚úì Inserted\n",
      "[ 229/518] ( 44.2%) Event 71940... ‚úì Inserted\n",
      "[ 229/518] ( 44.2%) Event 71940... ‚úì Inserted\n",
      "[ 230/518] ( 44.4%) Event 71941... ‚úì Inserted\n",
      "[ 230/518] ( 44.4%) Event 71941... ‚úì Inserted\n",
      "[ 231/518] ( 44.6%) Event 71948... ‚úì Inserted\n",
      "[ 231/518] ( 44.6%) Event 71948... ‚úì Inserted\n",
      "[ 232/518] ( 44.8%) Event 71957... ‚úì Inserted\n",
      "[ 232/518] ( 44.8%) Event 71957... ‚úì Inserted\n",
      "[ 233/518] ( 45.0%) Event 71959... ‚úì Inserted\n",
      "[ 233/518] ( 45.0%) Event 71959... ‚úì Inserted\n",
      "[ 234/518] ( 45.2%) Event 71962... ‚úì Inserted\n",
      "[ 234/518] ( 45.2%) Event 71962... ‚úì Inserted\n",
      "[ 235/518] ( 45.4%) Event 71963... ‚úì Inserted\n",
      "[ 235/518] ( 45.4%) Event 71963... ‚úì Inserted\n",
      "[ 236/518] ( 45.6%) Event 71968... ‚úì Inserted\n",
      "[ 236/518] ( 45.6%) Event 71968... ‚úì Inserted\n",
      "[ 237/518] ( 45.8%) Event 71969... ‚úì Inserted\n",
      "[ 237/518] ( 45.8%) Event 71969... ‚úì Inserted\n",
      "[ 238/518] ( 45.9%) Event 71973... ‚úì Inserted\n",
      "[ 238/518] ( 45.9%) Event 71973... ‚úì Inserted\n",
      "[ 239/518] ( 46.1%) Event 71975... ‚úì Inserted\n",
      "[ 239/518] ( 46.1%) Event 71975... ‚úì Inserted\n",
      "[ 240/518] ( 46.3%) Event 71976... ‚úì Inserted\n",
      "[ 240/518] ( 46.3%) Event 71976... ‚úì Inserted\n",
      "[ 241/518] ( 46.5%) Event 71979... ‚úì Inserted\n",
      "[ 241/518] ( 46.5%) Event 71979... ‚úì Inserted\n",
      "[ 242/518] ( 46.7%) Event 71980... ‚úì Inserted\n",
      "[ 242/518] ( 46.7%) Event 71980... ‚úì Inserted\n",
      "[ 243/518] ( 46.9%) Event 71982... ‚úì Inserted\n",
      "[ 243/518] ( 46.9%) Event 71982... ‚úì Inserted\n",
      "[ 244/518] ( 47.1%) Event 71984... ‚úì Inserted\n",
      "[ 244/518] ( 47.1%) Event 71984... ‚úì Inserted\n",
      "[ 245/518] ( 47.3%) Event 71986... ‚úì Inserted\n",
      "[ 245/518] ( 47.3%) Event 71986... ‚úì Inserted\n",
      "[ 246/518] ( 47.5%) Event 71987... ‚úì Inserted\n",
      "[ 246/518] ( 47.5%) Event 71987... ‚úì Inserted\n",
      "[ 247/518] ( 47.7%) Event 71989... ‚úì Inserted\n",
      "[ 247/518] ( 47.7%) Event 71989... ‚úì Inserted\n",
      "[ 248/518] ( 47.9%) Event 71994... ‚úì Inserted\n",
      "[ 248/518] ( 47.9%) Event 71994... ‚úì Inserted\n",
      "[ 249/518] ( 48.1%) Event 71995... ‚úì Inserted\n",
      "[ 249/518] ( 48.1%) Event 71995... ‚úì Inserted\n",
      "[ 250/518] ( 48.3%) Event 71997... ‚úì Inserted\n",
      "[ 250/518] ( 48.3%) Event 71997... ‚úì Inserted\n",
      "[ 251/518] ( 48.5%) Event 71999... ‚úì Inserted\n",
      "[ 251/518] ( 48.5%) Event 71999... ‚úì Inserted\n",
      "[ 252/518] ( 48.6%) Event 72001... ‚úì Inserted\n",
      "[ 252/518] ( 48.6%) Event 72001... ‚úì Inserted\n",
      "[ 253/518] ( 48.8%) Event 72008... ‚úì Inserted\n",
      "[ 253/518] ( 48.8%) Event 72008... ‚úì Inserted\n",
      "[ 254/518] ( 49.0%) Event 72009... ‚úì Inserted\n",
      "[ 254/518] ( 49.0%) Event 72009... ‚úì Inserted\n",
      "[ 255/518] ( 49.2%) Event 72010... ‚úì Inserted\n",
      "[ 255/518] ( 49.2%) Event 72010... ‚úì Inserted\n",
      "[ 256/518] ( 49.4%) Event 72011... ‚úì Inserted\n",
      "[ 256/518] ( 49.4%) Event 72011... ‚úì Inserted\n",
      "[ 257/518] ( 49.6%) Event 72012... ‚úì Inserted\n",
      "[ 257/518] ( 49.6%) Event 72012... ‚úì Inserted\n",
      "[ 258/518] ( 49.8%) Event 72013... ‚úì Inserted\n",
      "[ 258/518] ( 49.8%) Event 72013... ‚úì Inserted\n",
      "[ 259/518] ( 50.0%) Event 72015... ‚úì Inserted\n",
      "[ 259/518] ( 50.0%) Event 72015... ‚úì Inserted\n",
      "[ 260/518] ( 50.2%) Event 72020... ‚úì Inserted\n",
      "[ 260/518] ( 50.2%) Event 72020... ‚úì Inserted\n",
      "[ 261/518] ( 50.4%) Event 72024... ‚úì Inserted\n",
      "[ 261/518] ( 50.4%) Event 72024... ‚úì Inserted\n",
      "[ 262/518] ( 50.6%) Event 72026... ‚úì Inserted\n",
      "[ 262/518] ( 50.6%) Event 72026... ‚úì Inserted\n",
      "[ 263/518] ( 50.8%) Event 72027... ‚úì Inserted\n",
      "[ 263/518] ( 50.8%) Event 72027... ‚úì Inserted\n",
      "[ 264/518] ( 51.0%) Event 72028... ‚úì Inserted\n",
      "[ 264/518] ( 51.0%) Event 72028... ‚úì Inserted\n",
      "[ 265/518] ( 51.2%) Event 72032... ‚úì Inserted\n",
      "[ 265/518] ( 51.2%) Event 72032... ‚úì Inserted\n",
      "[ 266/518] ( 51.4%) Event 72033... ‚úì Inserted\n",
      "[ 266/518] ( 51.4%) Event 72033... ‚úì Inserted\n",
      "[ 267/518] ( 51.5%) Event 72034... ‚úì Inserted\n",
      "[ 267/518] ( 51.5%) Event 72034... ‚úì Inserted\n",
      "[ 268/518] ( 51.7%) Event 72035... ‚úì Inserted\n",
      "[ 268/518] ( 51.7%) Event 72035... ‚úì Inserted\n",
      "[ 269/518] ( 51.9%) Event 72042... ‚úì Inserted\n",
      "[ 269/518] ( 51.9%) Event 72042... ‚úì Inserted\n",
      "[ 270/518] ( 52.1%) Event 72051... ‚úì Inserted\n",
      "[ 270/518] ( 52.1%) Event 72051... ‚úì Inserted\n",
      "[ 271/518] ( 52.3%) Event 72052... ‚úì Inserted\n",
      "[ 271/518] ( 52.3%) Event 72052... ‚úì Inserted\n",
      "[ 272/518] ( 52.5%) Event 72067... ‚úì Inserted\n",
      "[ 272/518] ( 52.5%) Event 72067... ‚úì Inserted\n",
      "[ 273/518] ( 52.7%) Event 72071... ‚úì Inserted\n",
      "[ 273/518] ( 52.7%) Event 72071... ‚úì Inserted\n",
      "[ 274/518] ( 52.9%) Event 72074... ‚úì Inserted\n",
      "[ 274/518] ( 52.9%) Event 72074... ‚úì Inserted\n",
      "[ 275/518] ( 53.1%) Event 72079... ‚úì Inserted\n",
      "[ 275/518] ( 53.1%) Event 72079... ‚úì Inserted\n",
      "[ 276/518] ( 53.3%) Event 72080... ‚úì Inserted\n",
      "[ 276/518] ( 53.3%) Event 72080... ‚úì Inserted\n",
      "[ 277/518] ( 53.5%) Event 72081... ‚úì Inserted\n",
      "[ 277/518] ( 53.5%) Event 72081... ‚úì Inserted\n",
      "[ 278/518] ( 53.7%) Event 72082... ‚úì Inserted\n",
      "[ 278/518] ( 53.7%) Event 72082... ‚úì Inserted\n",
      "[ 279/518] ( 53.9%) Event 72084... ‚úì Inserted\n",
      "[ 279/518] ( 53.9%) Event 72084... ‚úì Inserted\n",
      "[ 280/518] ( 54.1%) Event 72085... ‚úì Inserted\n",
      "[ 280/518] ( 54.1%) Event 72085... ‚úì Inserted\n",
      "[ 281/518] ( 54.2%) Event 72088... ‚úì Inserted\n",
      "[ 281/518] ( 54.2%) Event 72088... ‚úì Inserted\n",
      "[ 282/518] ( 54.4%) Event 72089... ‚úì Inserted\n",
      "[ 282/518] ( 54.4%) Event 72089... ‚úì Inserted\n",
      "[ 283/518] ( 54.6%) Event 72094... ‚úì Inserted\n",
      "[ 283/518] ( 54.6%) Event 72094... ‚úì Inserted\n",
      "[ 284/518] ( 54.8%) Event 72096... ‚úì Inserted\n",
      "[ 284/518] ( 54.8%) Event 72096... ‚úì Inserted\n",
      "[ 285/518] ( 55.0%) Event 72100... ‚úì Inserted\n",
      "[ 285/518] ( 55.0%) Event 72100... ‚úì Inserted\n",
      "[ 286/518] ( 55.2%) Event 72101... ‚úì Inserted\n",
      "[ 286/518] ( 55.2%) Event 72101... ‚úì Inserted\n",
      "[ 287/518] ( 55.4%) Event 72104... ‚úì Inserted\n",
      "[ 287/518] ( 55.4%) Event 72104... ‚úì Inserted\n",
      "[ 288/518] ( 55.6%) Event 72105... ‚úì Inserted\n",
      "[ 288/518] ( 55.6%) Event 72105... ‚úì Inserted\n",
      "[ 289/518] ( 55.8%) Event 72106... ‚úì Inserted\n",
      "[ 289/518] ( 55.8%) Event 72106... ‚úì Inserted\n",
      "[ 290/518] ( 56.0%) Event 72108... ‚úì Inserted\n",
      "[ 290/518] ( 56.0%) Event 72108... ‚úì Inserted\n",
      "[ 291/518] ( 56.2%) Event 72109... ‚úì Inserted\n",
      "[ 291/518] ( 56.2%) Event 72109... ‚úì Inserted\n",
      "[ 292/518] ( 56.4%) Event 72110... ‚úì Inserted\n",
      "[ 292/518] ( 56.4%) Event 72110... ‚úì Inserted\n",
      "[ 293/518] ( 56.6%) Event 72111... ‚úì Inserted\n",
      "[ 293/518] ( 56.6%) Event 72111... ‚úì Inserted\n",
      "[ 294/518] ( 56.8%) Event 72112... ‚úì Inserted\n",
      "[ 294/518] ( 56.8%) Event 72112... ‚úì Inserted\n",
      "[ 295/518] ( 56.9%) Event 72120... ‚úì Inserted\n",
      "[ 295/518] ( 56.9%) Event 72120... ‚úì Inserted\n",
      "[ 296/518] ( 57.1%) Event 72128... ‚úì Inserted\n",
      "[ 296/518] ( 57.1%) Event 72128... ‚úì Inserted\n",
      "[ 297/518] ( 57.3%) Event 72140... ‚úì Inserted\n",
      "[ 297/518] ( 57.3%) Event 72140... ‚úì Inserted\n",
      "[ 298/518] ( 57.5%) Event 72141... ‚úì Inserted\n",
      "[ 298/518] ( 57.5%) Event 72141... ‚úì Inserted\n",
      "[ 299/518] ( 57.7%) Event 72144... ‚úì Inserted\n",
      "[ 299/518] ( 57.7%) Event 72144... ‚úì Inserted\n",
      "[ 300/518] ( 57.9%) Event 72145... ‚úì Inserted\n",
      "[ 300/518] ( 57.9%) Event 72145... ‚úì Inserted\n",
      "[ 301/518] ( 58.1%) Event 72147... ‚úì Inserted\n",
      "[ 301/518] ( 58.1%) Event 72147... ‚úì Inserted\n",
      "[ 302/518] ( 58.3%) Event 72153... ‚úì Inserted\n",
      "[ 302/518] ( 58.3%) Event 72153... ‚úì Inserted\n",
      "[ 303/518] ( 58.5%) Event 72155... ‚úì Inserted\n",
      "[ 303/518] ( 58.5%) Event 72155... ‚úì Inserted\n",
      "[ 304/518] ( 58.7%) Event 72160... ‚úì Inserted\n",
      "[ 304/518] ( 58.7%) Event 72160... ‚úì Inserted\n",
      "[ 305/518] ( 58.9%) Event 72162... ‚úì Inserted\n",
      "[ 305/518] ( 58.9%) Event 72162... ‚úì Inserted\n",
      "[ 306/518] ( 59.1%) Event 72169... ‚úì Inserted\n",
      "[ 306/518] ( 59.1%) Event 72169... ‚úì Inserted\n",
      "[ 307/518] ( 59.3%) Event 72171... ‚úì Inserted\n",
      "[ 307/518] ( 59.3%) Event 72171... ‚úì Inserted\n",
      "[ 308/518] ( 59.5%) Event 72173... ‚úì Inserted\n",
      "[ 308/518] ( 59.5%) Event 72173... ‚úì Inserted\n",
      "[ 309/518] ( 59.7%) Event 72177... ‚úì Inserted\n",
      "[ 309/518] ( 59.7%) Event 72177... ‚úì Inserted\n",
      "[ 310/518] ( 59.8%) Event 72180... ‚úì Inserted\n",
      "[ 310/518] ( 59.8%) Event 72180... ‚úì Inserted\n",
      "[ 311/518] ( 60.0%) Event 72181... ‚úì Inserted\n",
      "[ 311/518] ( 60.0%) Event 72181... ‚úì Inserted\n",
      "[ 312/518] ( 60.2%) Event 72192... ‚úì Inserted\n",
      "[ 312/518] ( 60.2%) Event 72192... ‚úì Inserted\n",
      "[ 313/518] ( 60.4%) Event 72194... ‚úì Inserted\n",
      "[ 313/518] ( 60.4%) Event 72194... ‚úì Inserted\n",
      "[ 314/518] ( 60.6%) Event 72204... ‚úì Inserted\n",
      "[ 314/518] ( 60.6%) Event 72204... ‚úì Inserted\n",
      "[ 315/518] ( 60.8%) Event 72209... ‚úì Inserted\n",
      "[ 315/518] ( 60.8%) Event 72209... ‚úì Inserted\n",
      "[ 316/518] ( 61.0%) Event 72210... ‚úì Inserted\n",
      "[ 316/518] ( 61.0%) Event 72210... ‚úì Inserted\n",
      "[ 317/518] ( 61.2%) Event 72211... ‚úì Inserted\n",
      "[ 317/518] ( 61.2%) Event 72211... ‚úì Inserted\n",
      "[ 318/518] ( 61.4%) Event 72212... ‚úì Inserted\n",
      "[ 318/518] ( 61.4%) Event 72212... ‚úì Inserted\n",
      "[ 319/518] ( 61.6%) Event 72215... ‚úì Inserted\n",
      "[ 319/518] ( 61.6%) Event 72215... ‚úì Inserted\n",
      "[ 320/518] ( 61.8%) Event 72221... ‚úì Inserted\n",
      "[ 320/518] ( 61.8%) Event 72221... ‚úì Inserted\n",
      "[ 321/518] ( 62.0%) Event 72226... ‚úì Inserted\n",
      "[ 321/518] ( 62.0%) Event 72226... ‚úì Inserted\n",
      "[ 322/518] ( 62.2%) Event 72229... ‚úì Inserted\n",
      "[ 322/518] ( 62.2%) Event 72229... ‚úì Inserted\n",
      "[ 323/518] ( 62.4%) Event 72230... ‚úì Inserted\n",
      "[ 323/518] ( 62.4%) Event 72230... ‚úì Inserted\n",
      "[ 324/518] ( 62.5%) Event 72232... ‚úì Inserted\n",
      "[ 324/518] ( 62.5%) Event 72232... ‚úì Inserted\n",
      "[ 325/518] ( 62.7%) Event 72236... ‚úì Inserted\n",
      "[ 325/518] ( 62.7%) Event 72236... ‚úì Inserted\n",
      "[ 326/518] ( 62.9%) Event 72238... ‚úì Inserted\n",
      "[ 326/518] ( 62.9%) Event 72238... ‚úì Inserted\n",
      "[ 327/518] ( 63.1%) Event 72241... ‚úì Inserted\n",
      "[ 327/518] ( 63.1%) Event 72241... ‚úì Inserted\n",
      "[ 328/518] ( 63.3%) Event 72242... ‚úì Inserted\n",
      "[ 328/518] ( 63.3%) Event 72242... ‚úì Inserted\n",
      "[ 329/518] ( 63.5%) Event 72243... ‚úì Inserted\n",
      "[ 329/518] ( 63.5%) Event 72243... ‚úì Inserted\n",
      "[ 330/518] ( 63.7%) Event 72244... ‚úì Inserted\n",
      "[ 330/518] ( 63.7%) Event 72244... ‚úì Inserted\n",
      "[ 331/518] ( 63.9%) Event 72245... ‚úì Inserted\n",
      "[ 331/518] ( 63.9%) Event 72245... ‚úì Inserted\n",
      "[ 332/518] ( 64.1%) Event 72246... ‚úì Inserted\n",
      "[ 332/518] ( 64.1%) Event 72246... ‚úì Inserted\n",
      "[ 333/518] ( 64.3%) Event 72248... ‚úì Inserted\n",
      "[ 333/518] ( 64.3%) Event 72248... ‚úì Inserted\n",
      "[ 334/518] ( 64.5%) Event 72250... ‚úì Inserted\n",
      "[ 334/518] ( 64.5%) Event 72250... ‚úì Inserted\n",
      "[ 335/518] ( 64.7%) Event 72251... ‚úì Inserted\n",
      "[ 335/518] ( 64.7%) Event 72251... ‚úì Inserted\n",
      "[ 336/518] ( 64.9%) Event 72253... ‚úì Inserted\n",
      "[ 336/518] ( 64.9%) Event 72253... ‚úì Inserted\n",
      "[ 337/518] ( 65.1%) Event 72254... ‚úì Inserted\n",
      "[ 337/518] ( 65.1%) Event 72254... ‚úì Inserted\n",
      "[ 338/518] ( 65.3%) Event 72260... ‚úì Inserted\n",
      "[ 338/518] ( 65.3%) Event 72260... ‚úì Inserted\n",
      "[ 339/518] ( 65.4%) Event 72261... ‚úì Inserted\n",
      "[ 339/518] ( 65.4%) Event 72261... ‚úì Inserted\n",
      "[ 340/518] ( 65.6%) Event 72266... ‚úì Inserted\n",
      "[ 340/518] ( 65.6%) Event 72266... ‚úì Inserted\n",
      "[ 341/518] ( 65.8%) Event 72267... ‚úì Inserted\n",
      "[ 341/518] ( 65.8%) Event 72267... ‚úì Inserted\n",
      "[ 342/518] ( 66.0%) Event 72268... ‚úì Inserted\n",
      "[ 342/518] ( 66.0%) Event 72268... ‚úì Inserted\n",
      "[ 343/518] ( 66.2%) Event 72269... ‚úì Inserted\n",
      "[ 343/518] ( 66.2%) Event 72269... ‚úì Inserted\n",
      "[ 344/518] ( 66.4%) Event 72271... ‚úì Inserted\n",
      "[ 344/518] ( 66.4%) Event 72271... ‚úì Inserted\n",
      "[ 345/518] ( 66.6%) Event 72273... ‚úì Inserted\n",
      "[ 345/518] ( 66.6%) Event 72273... ‚úì Inserted\n",
      "[ 346/518] ( 66.8%) Event 72274... ‚úì Inserted\n",
      "[ 346/518] ( 66.8%) Event 72274... ‚úì Inserted\n",
      "[ 347/518] ( 67.0%) Event 72275... ‚úì Inserted\n",
      "[ 347/518] ( 67.0%) Event 72275... ‚úì Inserted\n",
      "[ 348/518] ( 67.2%) Event 72278... ‚úì Inserted\n",
      "[ 348/518] ( 67.2%) Event 72278... ‚úì Inserted\n",
      "[ 349/518] ( 67.4%) Event 72282... ‚úì Inserted\n",
      "[ 349/518] ( 67.4%) Event 72282... ‚úì Inserted\n",
      "[ 350/518] ( 67.6%) Event 72283... ‚úì Inserted\n",
      "[ 350/518] ( 67.6%) Event 72283... ‚úì Inserted\n",
      "[ 351/518] ( 67.8%) Event 72284... ‚úì Inserted\n",
      "[ 351/518] ( 67.8%) Event 72284... ‚úì Inserted\n",
      "[ 352/518] ( 68.0%) Event 72286... ‚úì Inserted\n",
      "[ 352/518] ( 68.0%) Event 72286... ‚úì Inserted\n",
      "[ 353/518] ( 68.1%) Event 72287... ‚úì Inserted\n",
      "[ 353/518] ( 68.1%) Event 72287... ‚úì Inserted\n",
      "[ 354/518] ( 68.3%) Event 72289... ‚úì Inserted\n",
      "[ 354/518] ( 68.3%) Event 72289... ‚úì Inserted\n",
      "[ 355/518] ( 68.5%) Event 72291... ‚úì Inserted\n",
      "[ 355/518] ( 68.5%) Event 72291... ‚úì Inserted\n",
      "[ 356/518] ( 68.7%) Event 72293... ‚úì Inserted\n",
      "[ 356/518] ( 68.7%) Event 72293... ‚úì Inserted\n",
      "[ 357/518] ( 68.9%) Event 72295... ‚úì Inserted\n",
      "[ 357/518] ( 68.9%) Event 72295... ‚úì Inserted\n",
      "[ 358/518] ( 69.1%) Event 72300... ‚úì Inserted\n",
      "[ 358/518] ( 69.1%) Event 72300... ‚úì Inserted\n",
      "[ 359/518] ( 69.3%) Event 72303... ‚úì Inserted\n",
      "[ 359/518] ( 69.3%) Event 72303... ‚úì Inserted\n",
      "[ 360/518] ( 69.5%) Event 72309... ‚úì Inserted\n",
      "[ 360/518] ( 69.5%) Event 72309... ‚úì Inserted\n",
      "[ 361/518] ( 69.7%) Event 72312... ‚úì Inserted\n",
      "[ 361/518] ( 69.7%) Event 72312... ‚úì Inserted\n",
      "[ 362/518] ( 69.9%) Event 72314... ‚úì Inserted\n",
      "[ 362/518] ( 69.9%) Event 72314... ‚úì Inserted\n",
      "[ 363/518] ( 70.1%) Event 72317... ‚úì Inserted\n",
      "[ 363/518] ( 70.1%) Event 72317... ‚úì Inserted\n",
      "[ 364/518] ( 70.3%) Event 72318... ‚úì Inserted\n",
      "[ 364/518] ( 70.3%) Event 72318... ‚úì Inserted\n",
      "[ 365/518] ( 70.5%) Event 72319... ‚úì Inserted\n",
      "[ 365/518] ( 70.5%) Event 72319... ‚úì Inserted\n",
      "[ 366/518] ( 70.7%) Event 72322... ‚úì Inserted\n",
      "[ 366/518] ( 70.7%) Event 72322... ‚úì Inserted\n",
      "[ 367/518] ( 70.8%) Event 72323... ‚úì Inserted\n",
      "[ 367/518] ( 70.8%) Event 72323... ‚úì Inserted\n",
      "[ 368/518] ( 71.0%) Event 72325... ‚úì Inserted\n",
      "[ 368/518] ( 71.0%) Event 72325... ‚úì Inserted\n",
      "[ 369/518] ( 71.2%) Event 72326... ‚úì Inserted\n",
      "[ 369/518] ( 71.2%) Event 72326... ‚úì Inserted\n",
      "[ 370/518] ( 71.4%) Event 72328... ‚úì Inserted\n",
      "[ 370/518] ( 71.4%) Event 72328... ‚úì Inserted\n",
      "[ 371/518] ( 71.6%) Event 72330... ‚úì Inserted\n",
      "[ 371/518] ( 71.6%) Event 72330... ‚úì Inserted\n",
      "[ 372/518] ( 71.8%) Event 72331... ‚úì Inserted\n",
      "[ 372/518] ( 71.8%) Event 72331... ‚úì Inserted\n",
      "[ 373/518] ( 72.0%) Event 72338... ‚úì Inserted\n",
      "[ 373/518] ( 72.0%) Event 72338... ‚úì Inserted\n",
      "[ 374/518] ( 72.2%) Event 72343... ‚úì Inserted\n",
      "[ 374/518] ( 72.2%) Event 72343... ‚úì Inserted\n",
      "[ 375/518] ( 72.4%) Event 72345... ‚úì Inserted\n",
      "[ 375/518] ( 72.4%) Event 72345... ‚úì Inserted\n",
      "[ 376/518] ( 72.6%) Event 72346... ‚úì Inserted\n",
      "[ 376/518] ( 72.6%) Event 72346... ‚úì Inserted\n",
      "[ 377/518] ( 72.8%) Event 72347... ‚úì Inserted\n",
      "[ 377/518] ( 72.8%) Event 72347... ‚úì Inserted\n",
      "[ 378/518] ( 73.0%) Event 72351... ‚úì Inserted\n",
      "[ 378/518] ( 73.0%) Event 72351... ‚úì Inserted\n",
      "[ 379/518] ( 73.2%) Event 72352... ‚úì Inserted\n",
      "[ 379/518] ( 73.2%) Event 72352... ‚úì Inserted\n",
      "[ 380/518] ( 73.4%) Event 72353... ‚úì Inserted\n",
      "[ 380/518] ( 73.4%) Event 72353... ‚úì Inserted\n",
      "[ 381/518] ( 73.6%) Event 72355... ‚úì Inserted\n",
      "[ 381/518] ( 73.6%) Event 72355... ‚úì Inserted\n",
      "[ 382/518] ( 73.7%) Event 72357... ‚úì Inserted\n",
      "[ 382/518] ( 73.7%) Event 72357... ‚úì Inserted\n",
      "[ 383/518] ( 73.9%) Event 72359... ‚úì Inserted\n",
      "[ 383/518] ( 73.9%) Event 72359... ‚úì Inserted\n",
      "[ 384/518] ( 74.1%) Event 72360... ‚úì Inserted\n",
      "[ 384/518] ( 74.1%) Event 72360... ‚úì Inserted\n",
      "[ 385/518] ( 74.3%) Event 72366... ‚úì Inserted\n",
      "[ 385/518] ( 74.3%) Event 72366... ‚úì Inserted\n",
      "[ 386/518] ( 74.5%) Event 72367... ‚úì Inserted\n",
      "[ 386/518] ( 74.5%) Event 72367... ‚úì Inserted\n",
      "[ 387/518] ( 74.7%) Event 72369... ‚úì Inserted\n",
      "[ 387/518] ( 74.7%) Event 72369... ‚úì Inserted\n",
      "[ 388/518] ( 74.9%) Event 72370... ‚úì Inserted\n",
      "[ 388/518] ( 74.9%) Event 72370... ‚úì Inserted\n",
      "[ 389/518] ( 75.1%) Event 72375... ‚úì Inserted\n",
      "[ 389/518] ( 75.1%) Event 72375... ‚úì Inserted\n",
      "[ 390/518] ( 75.3%) Event 72378... ‚úì Inserted\n",
      "[ 390/518] ( 75.3%) Event 72378... ‚úì Inserted\n",
      "[ 391/518] ( 75.5%) Event 72379... ‚úì Inserted\n",
      "[ 391/518] ( 75.5%) Event 72379... ‚úì Inserted\n",
      "[ 392/518] ( 75.7%) Event 72380... ‚úì Inserted\n",
      "[ 392/518] ( 75.7%) Event 72380... ‚úì Inserted\n",
      "[ 393/518] ( 75.9%) Event 72381... ‚úì Inserted\n",
      "[ 393/518] ( 75.9%) Event 72381... ‚úì Inserted\n",
      "[ 394/518] ( 76.1%) Event 72382... ‚úì Inserted\n",
      "[ 394/518] ( 76.1%) Event 72382... ‚úì Inserted\n",
      "[ 395/518] ( 76.3%) Event 72385... ‚úì Inserted\n",
      "[ 395/518] ( 76.3%) Event 72385... ‚úì Inserted\n",
      "[ 396/518] ( 76.4%) Event 72386... ‚úì Inserted\n",
      "[ 396/518] ( 76.4%) Event 72386... ‚úì Inserted\n",
      "[ 397/518] ( 76.6%) Event 72387... ‚úì Inserted\n",
      "[ 397/518] ( 76.6%) Event 72387... ‚úì Inserted\n",
      "[ 398/518] ( 76.8%) Event 72388... ‚úì Inserted\n",
      "[ 398/518] ( 76.8%) Event 72388... ‚úì Inserted\n",
      "[ 399/518] ( 77.0%) Event 72389... ‚úì Inserted\n",
      "[ 399/518] ( 77.0%) Event 72389... ‚úì Inserted\n",
      "[ 400/518] ( 77.2%) Event 72392... ‚úì Inserted\n",
      "[ 400/518] ( 77.2%) Event 72392... ‚úì Inserted\n",
      "[ 401/518] ( 77.4%) Event 72393... ‚úì Inserted\n",
      "[ 401/518] ( 77.4%) Event 72393... ‚úì Inserted\n",
      "[ 402/518] ( 77.6%) Event 72394... ‚úì Inserted\n",
      "[ 402/518] ( 77.6%) Event 72394... ‚úì Inserted\n",
      "[ 403/518] ( 77.8%) Event 72395... ‚úì Inserted\n",
      "[ 403/518] ( 77.8%) Event 72395... ‚úì Inserted\n",
      "[ 404/518] ( 78.0%) Event 72396... ‚úì Inserted\n",
      "[ 404/518] ( 78.0%) Event 72396... ‚úì Inserted\n",
      "[ 405/518] ( 78.2%) Event 72397... ‚úì Inserted\n",
      "[ 405/518] ( 78.2%) Event 72397... ‚úì Inserted\n",
      "[ 406/518] ( 78.4%) Event 72398... ‚úì Inserted\n",
      "[ 406/518] ( 78.4%) Event 72398... ‚úì Inserted\n",
      "[ 407/518] ( 78.6%) Event 72399... ‚úì Inserted\n",
      "[ 407/518] ( 78.6%) Event 72399... ‚úì Inserted\n",
      "[ 408/518] ( 78.8%) Event 72400... ‚úì Inserted\n",
      "[ 408/518] ( 78.8%) Event 72400... ‚úì Inserted\n",
      "[ 409/518] ( 79.0%) Event 72401... ‚úì Inserted\n",
      "[ 409/518] ( 79.0%) Event 72401... ‚úì Inserted\n",
      "[ 410/518] ( 79.2%) Event 72403... ‚úì Inserted\n",
      "[ 410/518] ( 79.2%) Event 72403... ‚úì Inserted\n",
      "[ 411/518] ( 79.3%) Event 72404... ‚úì Inserted\n",
      "[ 411/518] ( 79.3%) Event 72404... ‚úì Inserted\n",
      "[ 412/518] ( 79.5%) Event 72405... ‚úì Inserted\n",
      "[ 412/518] ( 79.5%) Event 72405... ‚úì Inserted\n",
      "[ 413/518] ( 79.7%) Event 72406... ‚úì Inserted\n",
      "[ 413/518] ( 79.7%) Event 72406... ‚úì Inserted\n",
      "[ 414/518] ( 79.9%) Event 72409... ‚úì Inserted\n",
      "[ 414/518] ( 79.9%) Event 72409... ‚úì Inserted\n",
      "[ 415/518] ( 80.1%) Event 72411... ‚úì Inserted\n",
      "[ 415/518] ( 80.1%) Event 72411... ‚úì Inserted\n",
      "[ 416/518] ( 80.3%) Event 72415... ‚úì Inserted\n",
      "[ 416/518] ( 80.3%) Event 72415... ‚úì Inserted\n",
      "[ 417/518] ( 80.5%) Event 72420... ‚úì Inserted\n",
      "[ 417/518] ( 80.5%) Event 72420... ‚úì Inserted\n",
      "[ 418/518] ( 80.7%) Event 72423... ‚úì Inserted\n",
      "[ 418/518] ( 80.7%) Event 72423... ‚úì Inserted\n",
      "[ 419/518] ( 80.9%) Event 72424... ‚úì Inserted\n",
      "[ 419/518] ( 80.9%) Event 72424... ‚úì Inserted\n",
      "[ 420/518] ( 81.1%) Event 72425... ‚úì Inserted\n",
      "[ 420/518] ( 81.1%) Event 72425... ‚úì Inserted\n",
      "[ 421/518] ( 81.3%) Event 72427... ‚úì Inserted\n",
      "[ 421/518] ( 81.3%) Event 72427... ‚úì Inserted\n",
      "[ 422/518] ( 81.5%) Event 72428... ‚úì Inserted\n",
      "[ 422/518] ( 81.5%) Event 72428... ‚úì Inserted\n",
      "[ 423/518] ( 81.7%) Event 72431... ‚úì Inserted\n",
      "[ 423/518] ( 81.7%) Event 72431... ‚úì Inserted\n",
      "[ 424/518] ( 81.9%) Event 72433... ‚úì Inserted\n",
      "[ 424/518] ( 81.9%) Event 72433... ‚úì Inserted\n",
      "[ 425/518] ( 82.0%) Event 72435... ‚úì Inserted\n",
      "[ 425/518] ( 82.0%) Event 72435... ‚úì Inserted\n",
      "[ 426/518] ( 82.2%) Event 72436... ‚úì Inserted\n",
      "[ 426/518] ( 82.2%) Event 72436... ‚úì Inserted\n",
      "[ 427/518] ( 82.4%) Event 72437... ‚úì Inserted\n",
      "[ 427/518] ( 82.4%) Event 72437... ‚úì Inserted\n",
      "[ 428/518] ( 82.6%) Event 72438... ‚úì Inserted\n",
      "[ 428/518] ( 82.6%) Event 72438... ‚úì Inserted\n",
      "[ 429/518] ( 82.8%) Event 72439... ‚úì Inserted\n",
      "[ 429/518] ( 82.8%) Event 72439... ‚úì Inserted\n",
      "[ 430/518] ( 83.0%) Event 72440... ‚úì Inserted\n",
      "[ 430/518] ( 83.0%) Event 72440... ‚úì Inserted\n",
      "[ 431/518] ( 83.2%) Event 72443... ‚úì Inserted\n",
      "[ 431/518] ( 83.2%) Event 72443... ‚úì Inserted\n",
      "[ 432/518] ( 83.4%) Event 72445... ‚úì Inserted\n",
      "[ 432/518] ( 83.4%) Event 72445... ‚úì Inserted\n",
      "[ 433/518] ( 83.6%) Event 72447... ‚úì Inserted\n",
      "[ 433/518] ( 83.6%) Event 72447... ‚úì Inserted\n",
      "[ 434/518] ( 83.8%) Event 72450... ‚úì Inserted\n",
      "[ 434/518] ( 83.8%) Event 72450... ‚úì Inserted\n",
      "[ 435/518] ( 84.0%) Event 72451... ‚úì Inserted\n",
      "[ 435/518] ( 84.0%) Event 72451... ‚úì Inserted\n",
      "[ 436/518] ( 84.2%) Event 72453... ‚úì Inserted\n",
      "[ 436/518] ( 84.2%) Event 72453... ‚úì Inserted\n",
      "[ 437/518] ( 84.4%) Event 72455... ‚úì Inserted\n",
      "[ 437/518] ( 84.4%) Event 72455... ‚úì Inserted\n",
      "[ 438/518] ( 84.6%) Event 72458... ‚úì Inserted\n",
      "[ 438/518] ( 84.6%) Event 72458... ‚úì Inserted\n",
      "[ 439/518] ( 84.7%) Event 72459... ‚úì Inserted\n",
      "[ 439/518] ( 84.7%) Event 72459... ‚úì Inserted\n",
      "[ 440/518] ( 84.9%) Event 72460... ‚úì Inserted\n",
      "[ 440/518] ( 84.9%) Event 72460... ‚úì Inserted\n",
      "[ 441/518] ( 85.1%) Event 72461... ‚úì Inserted\n",
      "[ 441/518] ( 85.1%) Event 72461... ‚úì Inserted\n",
      "[ 442/518] ( 85.3%) Event 72462... ‚úì Inserted\n",
      "[ 442/518] ( 85.3%) Event 72462... ‚úì Inserted\n",
      "[ 443/518] ( 85.5%) Event 72465... ‚úì Inserted\n",
      "[ 443/518] ( 85.5%) Event 72465... ‚úì Inserted\n",
      "[ 444/518] ( 85.7%) Event 72467... ‚úì Inserted\n",
      "[ 444/518] ( 85.7%) Event 72467... ‚úì Inserted\n",
      "[ 445/518] ( 85.9%) Event 72471... ‚úì Inserted\n",
      "[ 445/518] ( 85.9%) Event 72471... ‚úì Inserted\n",
      "[ 446/518] ( 86.1%) Event 72472... ‚úì Inserted\n",
      "[ 446/518] ( 86.1%) Event 72472... ‚úì Inserted\n",
      "[ 447/518] ( 86.3%) Event 72473... ‚úì Inserted\n",
      "[ 447/518] ( 86.3%) Event 72473... ‚úì Inserted\n",
      "[ 448/518] ( 86.5%) Event 72475... ‚úì Inserted\n",
      "[ 448/518] ( 86.5%) Event 72475... ‚úì Inserted\n",
      "[ 449/518] ( 86.7%) Event 72476... ‚úì Inserted\n",
      "[ 449/518] ( 86.7%) Event 72476... ‚úì Inserted\n",
      "[ 450/518] ( 86.9%) Event 72477... ‚úì Inserted\n",
      "[ 450/518] ( 86.9%) Event 72477... ‚úì Inserted\n",
      "[ 451/518] ( 87.1%) Event 72478... ‚úì Inserted\n",
      "[ 451/518] ( 87.1%) Event 72478... ‚úì Inserted\n",
      "[ 452/518] ( 87.3%) Event 72479... ‚úì Inserted\n",
      "[ 452/518] ( 87.3%) Event 72479... ‚úì Inserted\n",
      "[ 453/518] ( 87.5%) Event 72480... ‚úì Inserted\n",
      "[ 453/518] ( 87.5%) Event 72480... ‚úì Inserted\n",
      "[ 454/518] ( 87.6%) Event 72482... ‚úì Inserted\n",
      "[ 454/518] ( 87.6%) Event 72482... ‚úì Inserted\n",
      "[ 455/518] ( 87.8%) Event 72484... ‚úì Inserted\n",
      "[ 455/518] ( 87.8%) Event 72484... ‚úì Inserted\n",
      "[ 456/518] ( 88.0%) Event 72486... ‚úì Inserted\n",
      "[ 456/518] ( 88.0%) Event 72486... ‚úì Inserted\n",
      "[ 457/518] ( 88.2%) Event 72489... ‚úì Inserted\n",
      "[ 457/518] ( 88.2%) Event 72489... ‚úì Inserted\n",
      "[ 458/518] ( 88.4%) Event 72490... ‚úì Inserted\n",
      "[ 458/518] ( 88.4%) Event 72490... ‚úì Inserted\n",
      "[ 459/518] ( 88.6%) Event 72493... ‚úì Inserted\n",
      "[ 459/518] ( 88.6%) Event 72493... ‚úì Inserted\n",
      "[ 460/518] ( 88.8%) Event 72494... ‚úì Inserted\n",
      "[ 460/518] ( 88.8%) Event 72494... ‚úì Inserted\n",
      "[ 461/518] ( 89.0%) Event 72495... ‚úì Inserted\n",
      "[ 461/518] ( 89.0%) Event 72495... ‚úì Inserted\n",
      "[ 462/518] ( 89.2%) Event 72498... ‚úì Inserted\n",
      "[ 462/518] ( 89.2%) Event 72498... ‚úì Inserted\n",
      "[ 463/518] ( 89.4%) Event 72499... ‚úì Inserted\n",
      "[ 463/518] ( 89.4%) Event 72499... ‚úì Inserted\n",
      "[ 464/518] ( 89.6%) Event 72501... ‚úì Inserted\n",
      "[ 464/518] ( 89.6%) Event 72501... ‚úì Inserted\n",
      "[ 465/518] ( 89.8%) Event 72503... ‚úì Inserted\n",
      "[ 465/518] ( 89.8%) Event 72503... ‚úì Inserted\n",
      "[ 466/518] ( 90.0%) Event 72505... ‚úì Inserted\n",
      "[ 466/518] ( 90.0%) Event 72505... ‚úì Inserted\n",
      "[ 467/518] ( 90.2%) Event 72506... ‚úì Inserted\n",
      "[ 467/518] ( 90.2%) Event 72506... ‚úì Inserted\n",
      "[ 468/518] ( 90.3%) Event 72507... ‚úì Inserted\n",
      "[ 468/518] ( 90.3%) Event 72507... ‚úì Inserted\n",
      "[ 469/518] ( 90.5%) Event 72509... ‚úì Inserted\n",
      "[ 469/518] ( 90.5%) Event 72509... ‚úì Inserted\n",
      "[ 470/518] ( 90.7%) Event 72510... ‚úì Inserted\n",
      "[ 470/518] ( 90.7%) Event 72510... ‚úì Inserted\n",
      "[ 471/518] ( 90.9%) Event 72511... ‚úì Inserted\n",
      "[ 471/518] ( 90.9%) Event 72511... ‚úì Inserted\n",
      "[ 472/518] ( 91.1%) Event 72512... ‚úì Inserted\n",
      "[ 472/518] ( 91.1%) Event 72512... ‚úì Inserted\n",
      "[ 473/518] ( 91.3%) Event 72513... ‚úì Inserted\n",
      "[ 473/518] ( 91.3%) Event 72513... ‚úì Inserted\n",
      "[ 474/518] ( 91.5%) Event 72514... ‚úì Inserted\n",
      "[ 474/518] ( 91.5%) Event 72514... ‚úì Inserted\n",
      "[ 475/518] ( 91.7%) Event 72515... ‚úì Inserted\n",
      "[ 475/518] ( 91.7%) Event 72515... ‚úì Inserted\n",
      "[ 476/518] ( 91.9%) Event 72516... ‚úì Inserted\n",
      "[ 476/518] ( 91.9%) Event 72516... ‚úì Inserted\n",
      "[ 477/518] ( 92.1%) Event 72520... ‚úì Inserted\n",
      "[ 477/518] ( 92.1%) Event 72520... ‚úì Inserted\n",
      "[ 478/518] ( 92.3%) Event 72522... ‚úì Inserted\n",
      "[ 478/518] ( 92.3%) Event 72522... ‚úì Inserted\n",
      "[ 479/518] ( 92.5%) Event 72523... ‚úì Inserted\n",
      "[ 479/518] ( 92.5%) Event 72523... ‚úì Inserted\n",
      "[ 480/518] ( 92.7%) Event 72524... ‚úì Inserted\n",
      "[ 480/518] ( 92.7%) Event 72524... ‚úì Inserted\n",
      "[ 481/518] ( 92.9%) Event 72525... ‚úì Inserted\n",
      "[ 481/518] ( 92.9%) Event 72525... ‚úì Inserted\n",
      "[ 482/518] ( 93.1%) Event 72527... ‚úì Inserted\n",
      "[ 482/518] ( 93.1%) Event 72527... ‚úì Inserted\n",
      "[ 483/518] ( 93.2%) Event 72528... ‚úì Inserted\n",
      "[ 483/518] ( 93.2%) Event 72528... ‚úì Inserted\n",
      "[ 484/518] ( 93.4%) Event 72529... ‚úì Inserted\n",
      "[ 484/518] ( 93.4%) Event 72529... ‚úì Inserted\n",
      "[ 485/518] ( 93.6%) Event 72530... ‚úì Inserted\n",
      "[ 485/518] ( 93.6%) Event 72530... ‚úì Inserted\n",
      "[ 486/518] ( 93.8%) Event 72532... ‚úì Inserted\n",
      "[ 486/518] ( 93.8%) Event 72532... ‚úì Inserted\n",
      "[ 487/518] ( 94.0%) Event 72533... ‚úì Inserted\n",
      "[ 487/518] ( 94.0%) Event 72533... ‚úì Inserted\n",
      "[ 488/518] ( 94.2%) Event 72534... ‚úì Inserted\n",
      "[ 488/518] ( 94.2%) Event 72534... ‚úì Inserted\n",
      "[ 489/518] ( 94.4%) Event 72536... ‚úì Inserted\n",
      "[ 489/518] ( 94.4%) Event 72536... ‚úì Inserted\n",
      "[ 490/518] ( 94.6%) Event 72537... ‚úì Inserted\n",
      "[ 490/518] ( 94.6%) Event 72537... ‚úì Inserted\n",
      "[ 491/518] ( 94.8%) Event 72538... ‚úì Inserted\n",
      "[ 491/518] ( 94.8%) Event 72538... ‚úì Inserted\n",
      "[ 492/518] ( 95.0%) Event 72539... ‚úì Inserted\n",
      "[ 492/518] ( 95.0%) Event 72539... ‚úì Inserted\n",
      "[ 493/518] ( 95.2%) Event 72541... ‚úì Inserted\n",
      "[ 493/518] ( 95.2%) Event 72541... ‚úì Inserted\n",
      "[ 494/518] ( 95.4%) Event 72542... ‚úì Inserted\n",
      "[ 494/518] ( 95.4%) Event 72542... ‚úì Inserted\n",
      "[ 495/518] ( 95.6%) Event 72543... ‚úì Inserted\n",
      "[ 495/518] ( 95.6%) Event 72543... ‚úì Inserted\n",
      "[ 496/518] ( 95.8%) Event 72544... ‚úì Inserted\n",
      "[ 496/518] ( 95.8%) Event 72544... ‚úì Inserted\n",
      "[ 497/518] ( 95.9%) Event 72546... ‚úì Inserted\n",
      "[ 497/518] ( 95.9%) Event 72546... ‚úì Inserted\n",
      "[ 498/518] ( 96.1%) Event 72547... ‚úì Inserted\n",
      "[ 498/518] ( 96.1%) Event 72547... ‚úì Inserted\n",
      "[ 499/518] ( 96.3%) Event 72550... ‚úì Inserted\n",
      "[ 499/518] ( 96.3%) Event 72550... ‚úì Inserted\n",
      "[ 500/518] ( 96.5%) Event 72551... ‚úì Inserted\n",
      "[ 500/518] ( 96.5%) Event 72551... ‚úì Inserted\n",
      "[ 501/518] ( 96.7%) Event 72552... ‚úì Inserted\n",
      "[ 501/518] ( 96.7%) Event 72552... ‚úì Inserted\n",
      "[ 502/518] ( 96.9%) Event 72553... ‚úì Inserted\n",
      "[ 502/518] ( 96.9%) Event 72553... ‚úì Inserted\n",
      "[ 503/518] ( 97.1%) Event 72557... ‚úì Inserted\n",
      "[ 503/518] ( 97.1%) Event 72557... ‚úì Inserted\n",
      "[ 504/518] ( 97.3%) Event 72561... ‚úì Inserted\n",
      "[ 504/518] ( 97.3%) Event 72561... ‚úì Inserted\n",
      "[ 505/518] ( 97.5%) Event 72567... ‚úì Inserted\n",
      "[ 505/518] ( 97.5%) Event 72567... ‚úì Inserted\n",
      "[ 506/518] ( 97.7%) Event 72568... ‚úì Inserted\n",
      "[ 506/518] ( 97.7%) Event 72568... ‚úì Inserted\n",
      "[ 507/518] ( 97.9%) Event 72569... ‚úì Inserted\n",
      "[ 507/518] ( 97.9%) Event 72569... ‚úì Inserted\n",
      "[ 508/518] ( 98.1%) Event 72570... ‚úì Inserted\n",
      "[ 508/518] ( 98.1%) Event 72570... ‚úì Inserted\n",
      "[ 509/518] ( 98.3%) Event 72572... ‚úì Inserted\n",
      "[ 509/518] ( 98.3%) Event 72572... ‚úì Inserted\n",
      "[ 510/518] ( 98.5%) Event 72578... ‚úì Inserted\n",
      "[ 510/518] ( 98.5%) Event 72578... ‚úì Inserted\n",
      "[ 511/518] ( 98.6%) Event 72579... ‚úì Inserted\n",
      "[ 511/518] ( 98.6%) Event 72579... ‚úì Inserted\n",
      "[ 512/518] ( 98.8%) Event 72581... ‚úì Inserted\n",
      "[ 512/518] ( 98.8%) Event 72581... ‚úì Inserted\n",
      "[ 513/518] ( 99.0%) Event 72584... ‚úì Inserted\n",
      "[ 513/518] ( 99.0%) Event 72584... ‚úì Inserted\n",
      "[ 514/518] ( 99.2%) Event 72599... ‚úì Inserted\n",
      "[ 514/518] ( 99.2%) Event 72599... ‚úì Inserted\n",
      "[ 515/518] ( 99.4%) Event 72605... ‚úì Inserted\n",
      "[ 515/518] ( 99.4%) Event 72605... ‚úì Inserted\n",
      "[ 516/518] ( 99.6%) Event 72608... ‚úì Inserted\n",
      "[ 516/518] ( 99.6%) Event 72608... ‚úì Inserted\n",
      "[ 517/518] ( 99.8%) Event 72612... ‚úì Inserted\n",
      "[ 517/518] ( 99.8%) Event 72612... ‚úì Inserted\n",
      "[ 518/518] (100.0%) Event 72613... ‚úì Inserted\n",
      "[ 518/518] (100.0%) Event 72613... ‚úì Inserted\n",
      "\n",
      "================================================================================\n",
      "MONGODB UPLOAD SUMMARY\n",
      "================================================================================\n",
      "Total processed: 518\n",
      "  ‚úì Inserted: 518\n",
      "  ‚Üª Updated: 0\n",
      "  ‚äò Skipped (already exists): 0\n",
      "  ‚úó Failed: 0\n",
      " ‚úì Inserted\n",
      "\n",
      "================================================================================\n",
      "MONGODB UPLOAD SUMMARY\n",
      "================================================================================\n",
      "Total processed: 518\n",
      "  ‚úì Inserted: 518\n",
      "  ‚Üª Updated: 0\n",
      "  ‚äò Skipped (already exists): 0\n",
      "  ‚úó Failed: 0\n",
      "\n",
      "üìä Database Statistics:\n",
      "   Total events in collection: 518\n",
      "   Database: recommendations-system\n",
      "   Collection: events\n",
      "\n",
      "üéâ UPLOAD COMPLETE!\n",
      "\n",
      "üìä Database Statistics:\n",
      "   Total events in collection: 518\n",
      "   Database: recommendations-system\n",
      "   Collection: events\n",
      "\n",
      "üéâ UPLOAD COMPLETE!\n",
      "‚úÖ MongoDB connection closed\n",
      "‚úÖ MongoDB connection closed\n"
     ]
    }
   ],
   "source": [
    "# Find the most recent events_detailed JSON file or load all individual events\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# First, try to load all individual event files from data/events/\n",
    "events_dir = DATA_DIR / 'events'\n",
    "all_events = []\n",
    "\n",
    "if events_dir.exists() and events_dir.is_dir():\n",
    "    print(f\"üìÅ Found events directory: {events_dir}\")\n",
    "    event_files = sorted(events_dir.glob('event_*.json'))\n",
    "    print(f\"üìñ Found {len(event_files)} individual event JSON files\\n\")\n",
    "    \n",
    "    # Load all individual event files\n",
    "    for event_file in event_files:\n",
    "        try:\n",
    "            with open(event_file, 'r', encoding='utf-8') as f:\n",
    "                event_data = json.load(f)\n",
    "                all_events.append(event_data)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {event_file.name}: {e}\")\n",
    "    \n",
    "    # Sort by event_id\n",
    "    all_events.sort(key=lambda x: int(x['event_id']))\n",
    "    print(f\"‚úÖ Loaded {len(all_events)} events from individual files\\n\")\n",
    "else:\n",
    "    # Fallback: load from combined events_detailed JSON file\n",
    "    json_files = sorted(DATA_DIR.glob('events_detailed_*.json'), key=os.path.getctime, reverse=True)\n",
    "    \n",
    "    if json_files:\n",
    "        latest_json = json_files[0]\n",
    "        print(f\"üìÑ Found events file: {latest_json.name}\")\n",
    "        print(f\"üìÖ Created: {dt.fromtimestamp(latest_json.stat().st_ctime)}\\n\")\n",
    "        \n",
    "        try:\n",
    "            with open(latest_json, 'r', encoding='utf-8') as f:\n",
    "                all_events = json.load(f)\n",
    "            print(f\"‚úÖ Loaded {len(all_events)} events from JSON\\n\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ùå Error reading JSON file: {e}\")\n",
    "            all_events = []\n",
    "    else:\n",
    "        print(\"‚ùå No event files found\")\n",
    "        print(\"   Please run Step 2 (comprehensive extraction) first\")\n",
    "        all_events = []\n",
    "\n",
    "if all_events:\n",
    "    # Initialize MongoDB connection\n",
    "    mongo_store = MongoDBEventStore(MONGODB_CONFIG)\n",
    "    \n",
    "    # Connect to MongoDB\n",
    "    if mongo_store.connect():\n",
    "        # Store events in MongoDB (skip existing by ID)\n",
    "        stats = mongo_store.store_events_batch(all_events, skip_existing=True)\n",
    "        \n",
    "        if stats:\n",
    "            print(f\"\\n\" + \"=\" * 80)\n",
    "            print(\"MONGODB UPLOAD SUMMARY\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"Total processed: {stats['total']}\")\n",
    "            print(f\"  ‚úì Inserted: {stats['inserted']}\")\n",
    "            print(f\"  ‚Üª Updated: {stats['updated']}\")\n",
    "            print(f\"  ‚äò Skipped (already exists): {stats['skipped']}\")\n",
    "            print(f\"  ‚úó Failed: {stats['failed']}\")\n",
    "            \n",
    "            # Get final statistics\n",
    "            db_stats = mongo_store.get_stats()\n",
    "            if db_stats:\n",
    "                print(f\"\\nüìä Database Statistics:\")\n",
    "                print(f\"   Total events in collection: {db_stats['total_documents']}\")\n",
    "                print(f\"   Database: {db_stats['database_name']}\")\n",
    "                print(f\"   Collection: {db_stats['collection_name']}\")\n",
    "            \n",
    "            print(f\"\\nüéâ UPLOAD COMPLETE!\")\n",
    "        else:\n",
    "            print(\"‚ùå Upload failed\")\n",
    "        \n",
    "        # Close connection\n",
    "        mongo_store.close()\n",
    "    else:\n",
    "        print(\"‚ùå Failed to connect to MongoDB\")\n",
    "        print(\"   Make sure MongoDB is running on localhost:27017\")\n",
    "        print(\"   Or update MONGODB_CONFIG with your MongoDB URI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "665b93dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÑÔ∏è Initializing MongoDB Event Store\n",
      "‚úÖ Connected to MongoDB\n",
      "   Database: recommendations-system\n",
      "   Collection: events\n",
      "‚úÖ Connected to MongoDB\n",
      "   Database: recommendations-system\n",
      "   Collection: events\n",
      "‚úÖ MongoDB connection closed\n",
      "‚úÖ Exported 518 documents to: c:\\Scrapping\\joinnus\\notebook\\data\\events_from_mongo_20251024_150003.json\n",
      "‚úÖ MongoDB connection closed\n",
      "‚úÖ Exported 518 documents to: c:\\Scrapping\\joinnus\\notebook\\data\\events_from_mongo_20251024_150003.json\n"
     ]
    }
   ],
   "source": [
    "from bson import json_util\n",
    "from datetime import datetime\n",
    "\n",
    "# Connect to MongoDB and dump entire collection to a JSON file (handles ObjectId/datetimes)\n",
    "mongo_store = MongoDBEventStore(MONGODB_CONFIG)\n",
    "if not mongo_store.connect():\n",
    "    raise SystemExit(\"‚ùå Failed to connect to MongoDB. Check MONGODB_CONFIG and network.\")\n",
    "\n",
    "docs = list(mongo_store.collection.find({}))\n",
    "mongo_store.close()\n",
    "\n",
    "out_file = DATA_DIR / f'events_from_mongo_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "with open(out_file, 'w', encoding='utf-8') as fh:\n",
    "    fh.write(json_util.dumps(docs, indent=2))\n",
    "\n",
    "print(f\"‚úÖ Exported {len(docs)} documents to: {out_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e5be6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
